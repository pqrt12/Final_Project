{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '../Resources/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "players_df = pd.read_csv(res_dir + 'players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(res_dir + 'team_completed_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Used to find the difference between team_1 and team_2 stat features\n",
    "\n",
    "#df2['kast'] = df2['1_kast']-df2['2_kast']\n",
    "#df2['adr'] = df2['1_adr']-df2['2_adr']\n",
    "#df2['KPR'] = df2['1_KPR']-df2['2_KPR']\n",
    "#df2['SPR'] = df2['1_SPR']-df2['2_SPR']\n",
    "#df2['DPR'] = df2['1_DPR']-df2['2_DPR']\n",
    "#df2['APR'] = df2['1_APR']-df2['2_APR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top CSGO Teams\n",
    "teams = ['AGO', 'forZe', 'AVANGAR', 'HAVU', 'Heroic', 'FURIA', 'Sprout',\n",
    "       'Spirit', 'Tricked', 'Windigo', 'Astralis', 'Liquid', 'TYLOO', 'FaZe',\n",
    "       'ALTERNATE aTTaX', 'Virtus.pro', 'Natus Vincere', 'North', 'NRG',\n",
    "       'Chiefs', 'ORDER', 'Renegades', 'Nemiga', 'mousesports', 'Valiance',\n",
    "       'G2', 'Singularity', 'ENCE', 'eUnited', 'LDLC', 'Grayhound',\n",
    "       'Complexity', 'BIG', 'DETONA', 'fnatic', 'NiP', 'SJ', 'HellRaisers',\n",
    "       'OpTic', 'Vitality', 'pro100', 'TeamOne', 'Cloud9', 'W7M',\n",
    "       'Movistar Riders', 'Epsilon', 'Chaos', 'PACT', 'MIBR', 'DreamEaters','Kinguin','Gambit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds games with only a top CSGO team in it\n",
    "teams_1 = df[df['team_1'].isin(teams)]\n",
    "teams_2 = df[df['team_2'].isin(teams)]\n",
    "df2 = pd.concat((teams_1,teams_2))\n",
    "df2 = df2.drop_duplicates()\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[['team_1','team_2','map_winner']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts team names it seperate categorical columns(team name columns)\n",
    "final = pd.get_dummies(df2, columns=['team_1','team_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for model\n",
    "X = final.copy()\n",
    "X = final.drop('map_winner', axis=1)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matches between chosen teams are doubled\n",
    "(13902, 955)\n",
    "\n",
    "without duplicated matches\n",
    "     if team_A beats team_B 5x, it shows up in the table 1 times.\n",
    "(5550, 955)\n",
    "\n",
    "with duplicated matches\n",
    "     if team_A beats team_B 5, it shows up in the table 5 times.\n",
    "(9950, 955)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target map_winner\n",
    "y = final['map_winner'].values\n",
    "# y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creating model statistics\n",
    "p_pred = model.predict_proba(X)\n",
    "y_pred = model.predict(X)\n",
    "score_ = model.score(X, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train=0.5954500494559841, test=0.5163610212153902\n",
    "train=0.6274774774774775, test=0.42882882882882883\n",
    "train=0.603140703517588, test=0.521608040201005\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model confusion matrix\n",
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction for future match\n",
    "model.predict(predict_final)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Probability of of each prediction\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(p_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output Learning Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LR model parameters\n",
    "filename = 'lr_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# X template df\n",
    "filename = 'template.csv'\n",
    "temp_X_df = pd.DataFrame(columns=X.columns)\n",
    "temp_X_df.to_csv(res_dir + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "model = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training SVM model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Predictions of first 30\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "   \"Prediction\": y_pred,\n",
    "   \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "predicted = model.predict(X_test)\n",
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train=0.7124359320205017, test=0.54297015462064\n",
    "train=0.7040540540540541, test=0.36396396396396397\n",
    "train=0.7228643216080402, test=0.4914572864321608\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output SVM model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SVM model parameters\n",
    "filename = 'svm_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randdom Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model and displaying model accuracy scores for both training and test\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=60,random_state=0) \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = rf.score(X_train, y_train)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train=0.680154662350508, test=0.5440489032722042\n",
    "train=0.7189189189189189, test=0.35135135135135137\n",
    "train=0.6913316582914573, test=0.5170854271356784\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction\n",
    "\n",
    "team_1 = ['Spirit']\n",
    "team_2 = ['Virtus.pro']\n",
    "map_winner = [None]\n",
    "\n",
    "predict = pd.DataFrame(list(zip(team_1,team_2,map_winner)),columns=['team_1','team_2','map_winner'])\n",
    "predict_final = pd.get_dummies(predict,columns=['team_1','team_2'])\n",
    "predict_final = predict_final.drop('map_winner',axis=1)\n",
    "predict_final"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "missing_cols = set(X.columns) - set(predict_final.columns)\n",
    "for c in missing_cols:\n",
    "    predict_final[c] = 0\n",
    "predict_final = predict_final[X.columns]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf.predict(predict_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output RandomForest Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RandomForest Classifier parameters\n",
    "filename = 'rf_classifier.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
