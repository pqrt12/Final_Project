{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '../Resources/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(res_dir + 'team_completed_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Finds games with only a top CSGO team in it\n",
    "teams_1 = df[df['team_1'].isin(teams)]\n",
    "teams_2 = df[df['team_2'].isin(teams)]\n",
    "df2 = pd.concat((teams_1,teams_2))\n",
    "# df2 = df2.drop_duplicates()\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare for team name encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['team_1','team_2','map_winner']]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_name_x = 0\n",
    "# covers upto thr, for example\n",
    "# thr = df2.shape[0] / 2\n",
    "def create_name_1(row):\n",
    "    global subsum\n",
    "    create_name_x += row['count']\n",
    "    if create_name_x < thr:\n",
    "        return row['team']\n",
    "    elif row['count'] < 2:\n",
    "        return 'oThErS'\n",
    "    else:\n",
    "        return 'oThErS_' + str(int(row['win_ratio'] * 10))\n",
    "\n",
    "# support most popular 52 teams.\n",
    "def create_name_2(row):\n",
    "    global create_name_x\n",
    "    create_name_x += 1\n",
    "    if create_name_x < 55:\n",
    "        return row['team']\n",
    "    elif row['count'] < 2:\n",
    "        return 'oThErS'\n",
    "    else:\n",
    "        return 'oThErS_' + str(int(row['win_ratio'] * 10))\n",
    "\n",
    "def team_df_transform(t_df):\n",
    "    global create_name_x\n",
    "    t_df.columns = ['team', 'win']\n",
    "    t_df = t_df.groupby('team').agg({'win': ['mean', 'count']})\n",
    "    t_df = t_df.reset_index()\n",
    "    t_df.columns = ['team', 'win_ratio', 'count']\n",
    "    t_df = t_df.sort_values(by=['count'], ascending=False)\n",
    "    #\n",
    "    create_name_x = 0\n",
    "    t_df['new_name'] = t_df.apply(lambda row: create_name_2(row), axis='columns')\n",
    "    return t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team1 map_win \n",
    "t_df = df2[['team_1', 'map_winner']].copy()\n",
    "arr = np.array(t_df['map_winner'].tolist())\n",
    "t_df['map_winner'] = list(2 - arr)\n",
    "#\n",
    "t1_df = team_df_transform(t_df)\n",
    "t1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team2 map_win \n",
    "t_df = df2[['team_2', 'map_winner']].copy()\n",
    "arr = np.array(t_df['map_winner'].tolist())\n",
    "t_df['map_winner'] = list(arr - 1)\n",
    "#\n",
    "t2_df = team_df_transform(t_df)\n",
    "t2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  merge to teamname encoding df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = t1_df.merge(t2_df, on='team', how='outer')\n",
    "team_df.columns = ['team',\n",
    "                   't1_win_ratio', 't1_count', 't1_name',\n",
    "                   't2_win_ratio', 't2_count', 't2_name']\n",
    "team_df.sort_values(by=['team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_names = {'t1_name': 'oThErS', 't2_name': 'oThErS'}\n",
    "team_df = team_df.fillna(value=def_names)\n",
    "team_encode_df = team_df[['team', 't1_name', 't2_name']]\n",
    "team_encode_df = team_encode_df.set_index('team')\n",
    "team_encode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_teams = team_encode_df.index.tolist()\n",
    "len(supported_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.columns = ['t1','t2','map_winner']\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to X teamname\n",
    "def create_X_name(row):\n",
    "    # default unknown\n",
    "    t1 = t2 = 'oThErS'\n",
    "    # team_1\n",
    "    t = row['t1']\n",
    "    if t in supported_teams:\n",
    "        t1 = team_encode_df.loc[t, 't1_name']\n",
    "    # team_2\n",
    "    t = row['t2']\n",
    "    if t in supported_teams:\n",
    "        t2 = team_encode_df.loc[t, 't2_name']\n",
    "    return [t1, t2]\n",
    "df3[['team_1', 'team_2']] = df3.apply(lambda row: create_X_name(row), axis='columns', result_type='expand')\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df3[['team_1','team_2','map_winner']]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['team_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### done dataframe preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts team names it seperate categorical columns(team name columns)\n",
    "final = pd.get_dummies(df2, columns=['team_1', 'team_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for model\n",
    "X = final.copy()\n",
    "X = final.drop('map_winner', axis=1)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, matches between chosen teams are doubled (baseline)\n",
    "(13902, 955)\n",
    "\n",
    "2, without duplicated matches\n",
    "     if team_A beats team_B 5x, it shows up in the table 1 times.\n",
    "(5550, 955)\n",
    "\n",
    "3, with duplicated matches\n",
    "     if team_A beats team_B 5, it shows up in the table 5 times.\n",
    "(9950, 955)\n",
    "\n",
    "4, with duplicated matches, group non-chosen teams to \"oThErS\"\n",
    "(9950, 106)\n",
    "\n",
    "5, expand to 100 teams, no groupping.\n",
    "(12982, 1233)\n",
    "\n",
    "6, expand to 100 teams, with groupping.\n",
    "(12982, 202)\n",
    "\n",
    "7, expand to 75 teams, with grouping.\n",
    "(11577, 152)\n",
    "\n",
    "8, double matches betw chosen teams, grouping others\n",
    "\n",
    "9, grouping to multiple bins, no double entries.\n",
    "(17345, 130)\n",
    "\n",
    "10, same as 9, change split random_state from 0 to 100.\n",
    "    not sensitive to date split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target map_winner\n",
    "y = final['map_winner'].values\n",
    "# y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, train_size=0.80)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creating model statistics\n",
    "p_pred = model.predict_proba(X)\n",
    "y_pred = model.predict(X)\n",
    "score_ = model.score(X, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.5954500494559841, test=0.5163610212153902\n",
    "2, train=0.6274774774774775, test=0.42882882882882883\n",
    "3, train=0.603140703517588, test=0.521608040201005\n",
    "4, train=0.5449748743718593, test=0.5231155778894472\n",
    "5, train=0.6074145402022147, test=0.5005775895263765\n",
    "6, train=0.5568608570052961, test=0.5028879476318829\n",
    "7, train=0.5550156570564734, test=0.4991364421416235\n",
    "8, train=0.5520187033540149, test=0.5411722402013665\n",
    "9, train=0.5875612568463534, test=0.5797059671375036\n",
    "10, train=0.5930383395791294, test=0.5794176996252522"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model confusion matrix\n",
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction for future match\n",
    "model.predict(predict_final)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Probability of of each prediction\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(p_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output Learning Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LR model parameters\n",
    "filename = 'lr_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# X template df\n",
    "filename = 'template.csv'\n",
    "temp_X_df = pd.DataFrame(columns=X.columns)\n",
    "temp_X_df.to_csv(res_dir + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "model = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training SVM model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Predictions of first 30\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "   \"Prediction\": y_pred,\n",
    "   \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "predicted = model.predict(X_test)\n",
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.7124359320205017, test=0.54297015462064\n",
    "2, train=0.7040540540540541, test=0.36396396396396397\n",
    "3, train=0.7228643216080402, test=0.4914572864321608\n",
    "4, train=0.610678391959799, test=0.5045226130653266\n",
    "5, train=0.7220991815117959, test=0.4874855602618406\n",
    "6, train=0.6363023591718825, test=0.496726992683866\n",
    "7, train=0.6178598423496383, test=0.4930915371329879\n",
    "8, train=0.6329466774570632, test=0.5609492988133765\n",
    "9, train=0.6439896223695589, test=0.5664456615739406\n",
    "10, train=0.6424041510521764, test=0.564427788988181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output SVM model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SVM model parameters\n",
    "filename = 'svm_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model and displaying model accuracy scores for both training and test\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=60,random_state=0) \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = rf.score(X_train, y_train)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.680154662350508, test=0.5440489032722042\n",
    "2, train=0.7189189189189189, test=0.35135135135135137\n",
    "3, train=0.6913316582914573, test=0.5170854271356784\n",
    "4, train=0.6201005025125628, test=0.5020100502512563\n",
    "5, train=0.6725084256138661, test=0.5059684251058915\n",
    "6, train=0.6527684159845931, test=0.4909510974201001\n",
    "7, train=0.6287657920310982, test=0.5056131260794473\n",
    "8, train=0.6359140365075083, test=0.556274721323265\n",
    "9, train=0.6523493802248487, test=0.5658691265494379\n",
    "10, train=0.6511963101758432, test=0.5647160565004324"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction\n",
    "\n",
    "team_1 = ['Spirit']\n",
    "team_2 = ['Virtus.pro']\n",
    "map_winner = [None]\n",
    "\n",
    "predict = pd.DataFrame(list(zip(team_1,team_2,map_winner)),columns=['team_1','team_2','map_winner'])\n",
    "predict_final = pd.get_dummies(predict,columns=['team_1','team_2'])\n",
    "predict_final = predict_final.drop('map_winner',axis=1)\n",
    "predict_final"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "missing_cols = set(X.columns) - set(predict_final.columns)\n",
    "for c in missing_cols:\n",
    "    predict_final[c] = 0\n",
    "predict_final = predict_final[X.columns]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf.predict(predict_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output RandomForest Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RandomForest Classifier parameters\n",
    "filename = 'rf_classifier.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
