{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '../Resources/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "players_df = pd.read_csv(res_dir + 'players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(res_dir + 'team_completed_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Used to find the difference between team_1 and team_2 stat features\n",
    "\n",
    "#df2['kast'] = df2['1_kast']-df2['2_kast']\n",
    "#df2['adr'] = df2['1_adr']-df2['2_adr']\n",
    "#df2['KPR'] = df2['1_KPR']-df2['2_KPR']\n",
    "#df2['SPR'] = df2['1_SPR']-df2['2_SPR']\n",
    "#df2['DPR'] = df2['1_DPR']-df2['2_DPR']\n",
    "#df2['APR'] = df2['1_APR']-df2['2_APR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t_df = df.groupby('team_1')['team_1'].count()\n",
    "t_df = pd.DataFrame(t_df)\n",
    "t_df.columns = ['count']\n",
    "t_df = t_df.reset_index()\n",
    "t_df = t_df.sort_values(by=['count'], ascending=False)\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t_df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# top 75 teams\n",
    "tt_df = t_df.head(75)\n",
    "tt_df['count'].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "teams = tt_df['team_1'].tolist()\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "75 teams\n",
    "['mousesports', 'Virtus.pro', 'fnatic', 'pro100', 'Tricked', 'forZe', 'Spirit', 'NiP', 'North', 'Windigo',\n",
    " 'Renegades', 'Sprout', 'Singularity', 'Natus Vincere', 'NRG', 'Liquid', 'OpTic', 'Valiance', 'SK',\n",
    " 'TeamOne', 'Heroic', 'eUnited', 'Tainted Minds', 'ORDER', 'Space Soldiers', 'G2', 'HAVU', 'TYLOO',\n",
    " 'SJ', 'Nemiga', 'Vitality', 'Luminosity', 'Grayhound', 'ViCi', 'x-kom', 'PRIDE', 'Rogue', 'LDLC',\n",
    " 'HellRaisers', 'eXtatus', 'Movistar Riders', 'Kinguin', 'Red Reserve', 'W7M', 'Ground Zero', 'FaZe',\n",
    " 'MIBR', 'Nexus', 'Winstrike', 'PACT', 'MVP PK', 'GODSENT', 'Vega Squadron', 'Quantum Bellator Fire',\n",
    " 'Legacy', 'Swole Patrol', 'AVANGAR', 'Mythic', 'iGame.com', 'Imperial', 'Isurus', 'FURIA', 'aAa',\n",
    " 'expert', 'Chiefs', 'Syman', 'Unique', 'Paradox', 'INTZ', 'x6tence Galaxy', 'Gambit', 'Yeah',\n",
    " 'Fragsters', 'Sharks', 'Izako Boars']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "100 teams\n",
    "['mousesports', 'Virtus.pro', 'fnatic', 'pro100', 'Tricked', 'forZe', 'Spirit', 'NiP', 'North', 'Windigo',\n",
    " 'Renegades', 'Sprout', 'Singularity', 'Natus Vincere', 'NRG', 'Liquid', 'OpTic', 'Valiance', 'SK',\n",
    " 'TeamOne', 'Heroic', 'eUnited', 'Tainted Minds', 'ORDER', 'Space Soldiers', 'G2', 'HAVU', 'TYLOO',\n",
    " 'SJ', 'Nemiga', 'Vitality', 'Luminosity', 'Grayhound', 'ViCi', 'x-kom', 'PRIDE', 'Rogue', 'LDLC',\n",
    " 'HellRaisers', 'eXtatus', 'Movistar Riders', 'Kinguin', 'Red Reserve', 'W7M', 'Ground Zero', 'FaZe',\n",
    " 'MIBR', 'Nexus', 'Winstrike', 'PACT', 'MVP PK', 'GODSENT', 'Vega Squadron', 'Quantum Bellator Fire',\n",
    " 'Legacy', 'Swole Patrol', 'AVANGAR', 'Mythic', 'iGame.com', 'Imperial', 'Isurus', 'FURIA', 'aAa',\n",
    " 'expert', 'Chiefs', 'Syman', 'Unique', 'Paradox', 'INTZ', 'x6tence Galaxy', 'Gambit', 'Yeah',\n",
    " 'Fragsters', 'Sharks', 'Izako Boars', 'SYF', 'Splyce', 'madlikewizards', 'Rise Nation', 'AGO',\n",
    " 'Epsilon', 'Ghost', 'Nordavind', 'TheMongolz', 'Envy', 'Complexity', 'Signature', 'EURONICS',\n",
    " 'Recca', 'subtLe', 'Wild', 'Flash', 'paiN', 'Taboo', 'fnatic Academy', 'BIG', 'North Academy',\n",
    " 'ENCE', 'Cloud9', 'DreamEaters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top CSGO Teams (52)\n",
    "teams = ['AGO', 'forZe', 'AVANGAR', 'HAVU', 'Heroic', 'FURIA', 'Sprout',\n",
    "       'Spirit', 'Tricked', 'Windigo', 'Astralis', 'Liquid', 'TYLOO', 'FaZe',\n",
    "       'ALTERNATE aTTaX', 'Virtus.pro', 'Natus Vincere', 'North', 'NRG',\n",
    "       'Chiefs', 'ORDER', 'Renegades', 'Nemiga', 'mousesports', 'Valiance',\n",
    "       'G2', 'Singularity', 'ENCE', 'eUnited', 'LDLC', 'Grayhound',\n",
    "       'Complexity', 'BIG', 'DETONA', 'fnatic', 'NiP', 'SJ', 'HellRaisers',\n",
    "       'OpTic', 'Vitality', 'pro100', 'TeamOne', 'Cloud9', 'W7M',\n",
    "       'Movistar Riders', 'Epsilon', 'Chaos', 'PACT', 'MIBR', 'DreamEaters','Kinguin','Gambit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds games with only a top CSGO team in it\n",
    "teams_1 = df[df['team_1'].isin(teams)]\n",
    "teams_2 = df[df['team_2'].isin(teams)]\n",
    "df2 = pd.concat((teams_1,teams_2))\n",
    "# df2 = df2.drop_duplicates()\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[['team_1','team_2','map_winner']]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# worst !!!\n",
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.columns = ['t1','t2','map_winner']\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to X teamname\n",
    "def create_X_name(row):\n",
    "    # default unknown\n",
    "    t1 = t2 = 'oThErS'\n",
    "    # team_1\n",
    "    t = row['t1']\n",
    "    if t in teams:\n",
    "        t1 = t\n",
    "    # team_2\n",
    "    t = row['t2']\n",
    "    if t in teams:\n",
    "        t2 = t\n",
    "    return [t1, t2]\n",
    "df3[['team_1', 'team_2']] = df3.apply(lambda row: create_X_name(row), axis='columns', result_type='expand')\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df3[['team_1','team_2','map_winner']]\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts team names it seperate categorical columns(team name columns)\n",
    "final = pd.get_dummies(df2, columns=['team_1','team_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for model\n",
    "X = final.copy()\n",
    "X = final.drop('map_winner', axis=1)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, matches between chosen teams are doubled (baseline)\n",
    "(13902, 955)\n",
    "\n",
    "2, without duplicated matches\n",
    "     if team_A beats team_B 5x, it shows up in the table 1 times.\n",
    "(5550, 955)\n",
    "\n",
    "3, with duplicated matches\n",
    "     if team_A beats team_B 5, it shows up in the table 5 times.\n",
    "(9950, 955)\n",
    "\n",
    "4, with duplicated matches, group non-chosen teams to \"oThErS\"\n",
    "(9950, 106)\n",
    "\n",
    "5, expand to 100 teams, no groupping.\n",
    "(12982, 1233)\n",
    "\n",
    "6, expand to 100 teams, with groupping.\n",
    "(12982, 202)\n",
    "\n",
    "7, expand to 75 teams, with grouping.\n",
    "(11577, 152)\n",
    "\n",
    "8, double matches betw chosen teams, grouping others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target map_winner\n",
    "y = final['map_winner'].values\n",
    "# y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training linear logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creating model statistics\n",
    "p_pred = model.predict_proba(X)\n",
    "y_pred = model.predict(X)\n",
    "score_ = model.score(X, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.5954500494559841, test=0.5163610212153902\n",
    "2, train=0.6274774774774775, test=0.42882882882882883\n",
    "3, train=0.603140703517588, test=0.521608040201005\n",
    "4, train=0.5449748743718593, test=0.5231155778894472\n",
    "5, train=0.6074145402022147, test=0.5005775895263765\n",
    "6, train=0.5568608570052961, test=0.5028879476318829\n",
    "7, train=0.5550156570564734, test=0.4991364421416235\n",
    "8, train=0.5520187033540149, test=0.5411722402013665"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model confusion matrix\n",
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction for future match\n",
    "model.predict(predict_final)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Probability of of each prediction\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(p_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output Learning Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LR model parameters\n",
    "filename = 'lr_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# X template df\n",
    "filename = 'template.csv'\n",
    "temp_X_df = pd.DataFrame(columns=X.columns)\n",
    "temp_X_df.to_csv(res_dir + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "model = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training SVM model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Predictions of first 30\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "   \"Prediction\": y_pred,\n",
    "   \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "predicted = model.predict(X_test)\n",
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = model.score(X_train, y_train)\n",
    "score2 = model.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.7124359320205017, test=0.54297015462064\n",
    "2, train=0.7040540540540541, test=0.36396396396396397\n",
    "3, train=0.7228643216080402, test=0.4914572864321608\n",
    "4, train=0.610678391959799, test=0.5045226130653266\n",
    "5, train=0.7220991815117959, test=0.4874855602618406\n",
    "6, train=0.6363023591718825, test=0.496726992683866\n",
    "7, train=0.6178598423496383, test=0.4930915371329879\n",
    "8, train=0.6329466774570632, test=0.5609492988133765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output SVM model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SVM model parameters\n",
    "filename = 'svm_model.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(model, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model and displaying model accuracy scores for both training and test\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=60,random_state=0) \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy Score\n",
    "score1 = rf.score(X_train, y_train)\n",
    "score2 = rf.score(X_test, y_test)\n",
    "print(f'train={score1}, test={score2}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1, train=0.680154662350508, test=0.5440489032722042\n",
    "2, train=0.7189189189189189, test=0.35135135135135137\n",
    "3, train=0.6913316582914573, test=0.5170854271356784\n",
    "4, train=0.6201005025125628, test=0.5020100502512563\n",
    "5, train=0.6725084256138661, test=0.5059684251058915\n",
    "6, train=0.6527684159845931, test=0.4909510974201001\n",
    "7, train=0.6287657920310982, test=0.5056131260794473\n",
    "8, train=0.6359140365075083, test=0.556274721323265"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example prediction\n",
    "\n",
    "team_1 = ['Spirit']\n",
    "team_2 = ['Virtus.pro']\n",
    "map_winner = [None]\n",
    "\n",
    "predict = pd.DataFrame(list(zip(team_1,team_2,map_winner)),columns=['team_1','team_2','map_winner'])\n",
    "predict_final = pd.get_dummies(predict,columns=['team_1','team_2'])\n",
    "predict_final = predict_final.drop('map_winner',axis=1)\n",
    "predict_final"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "missing_cols = set(X.columns) - set(predict_final.columns)\n",
    "for c in missing_cols:\n",
    "    predict_final[c] = 0\n",
    "predict_final = predict_final[X.columns]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf.predict(predict_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output RandomForest Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RandomForest Classifier parameters\n",
    "filename = 'rf_classifier.pkl'\n",
    "with open(res_dir + filename, 'wb') as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
