{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a filename (with path) such that it is accessible.\n",
    "def get_fullname(filename):\n",
    "    # as long as it is accessible.\n",
    "    if os.path.isfile(filename):\n",
    "        return filename\n",
    "\n",
    "    # search current working directory.\n",
    "    cur_working_dir = os.getcwd()\n",
    "    for i in range(50):\n",
    "        for (path, dir, files) in os.walk(cur_working_dir):\n",
    "            if filename in files:\n",
    "                return os.path.join(path, filename)\n",
    "\n",
    "        # check parent directory\n",
    "        parent = os.path.dirname(cur_working_dir)\n",
    "        if cur_working_dir == parent:\n",
    "            break;\n",
    "        cur_working_dir = parent\n",
    "\n",
    "    # did not found, simply return.\n",
    "    print(f\"file {filename} not found !!!\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Initiate headless driver for deployment\n",
    "browser = Browser(\"chrome\", executable_path=get_fullname(\"chromedriver.exe\"), headless=True)\n",
    "#browser = Browser('chrome', executable_path=get_fullname(\"chromedriver.exe\"))\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url\n",
    "hltv_url = 'https://www.hltv.org'\n",
    "output_filename = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_perf_table = 'perf_table'\n",
    "key_AWP_kills = 'AWP_kills'\n",
    "## top level\n",
    "key_timestamp = 'ts'\n",
    "key_map_stats = 'map_stats'\n",
    "key_teams = 'teams'\n",
    "key_player_st = 'match_player_st'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visist '/stats/matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# href=\"/stats/teams/4471/bemyfRAG\"\n",
    "# href=\"/stats/players/15090/PwnAlone\"\n",
    "def get_team_id(a):\n",
    "    sl = a['href'].split('/')\n",
    "    return sl[3]\n",
    "get_player_id = get_team_id\n",
    "\n",
    "def get_team_name(a):\n",
    "    sl = a['href'].split('/')\n",
    "    return sl[4]\n",
    "get_player_name = get_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only timestamp, map (if single map) and href are used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_get(browser, start, stop, next_url = '/stats/matches'):\n",
    "    # start is the newer match data, larger value\n",
    "    if start < stop:\n",
    "        t = stop\n",
    "        stop = start\n",
    "        start = t\n",
    "\n",
    "    start_url = False\n",
    "\n",
    "    # Visit the site to Scrape\n",
    "    ts = 0\n",
    "    all_done = False\n",
    "    match_urls = []\n",
    "\n",
    "    for cnt in range(10000):\n",
    "        # Visit the site to Scrape\n",
    "        browser.visit(hltv_url + next_url)\n",
    "        browser.is_element_present_by_tag('div.contentCol', wait_time=3)\n",
    "        content = BeautifulSoup(browser.html, 'html.parser').find('div', attrs={\"class\": \"contentCol\"})\n",
    "\n",
    "        # all matches in this page\n",
    "        trs = content.tbody.find_all('tr')\n",
    "\n",
    "        for tr in trs:\n",
    "            match_info= {}\n",
    "\n",
    "            # data-unix\n",
    "            td = tr.find('td', attrs={'class': 'date-col'})\n",
    "            t = int(td.div['data-unix'])/1000\n",
    "            if (t > start):\n",
    "                continue\n",
    "            if (t <= stop):\n",
    "                all_done = True\n",
    "                str = datetime.utcfromtimestamp(t).strftime('%Y/%m/%d %H:%M:%S')\n",
    "                print(f'Excluded 1st match timestamp: {str}')\n",
    "                break\n",
    "\n",
    "            if not start_url:\n",
    "                start_url = True\n",
    "                str = datetime.utcfromtimestamp(t).strftime('%Y/%m/%d %H:%M:%S')\n",
    "                print(f'Included 1st match timestamp: {str}')\n",
    "            match_info['href'] = td.a['href']\n",
    "            match_info['ts'] = t\n",
    "            # map\n",
    "            match_info['map'] = tr.find('div', attrs={'class': 'dynamic-map-name-full'}).text\n",
    "            match_urls.append(match_info)        \n",
    "\n",
    "        # keep current page\n",
    "        # if in middle of the page, may overlapping, prevent missing matches\n",
    "        if all_done:\n",
    "            break\n",
    "        # ========================================================================\n",
    "        # next page\n",
    "        # ========================================================================\n",
    "        # next page\n",
    "        a = content.find('a', attrs={\"class\": \"pagination-next\"}, href=True)\n",
    "        if not a:\n",
    "            print(f'Done {len(match_urls)} matches scraping.')\n",
    "            break\n",
    "        next_url = a['href']\n",
    "\n",
    "        # only during development\n",
    "        if cnt == 1000:\n",
    "            print(f'Truncate the match lists !!!!!')\n",
    "            print(f'Change next_url from: {next_url}')        \n",
    "            next_url = '/stats/matches?offset=79700'\n",
    "            print(f'                  to: {next_url}')  \n",
    "    \n",
    "    # return all urls and next_page_url\n",
    "    return match_urls, next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### player stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_get_(str):\n",
    "    # 'nn.nn%'\n",
    "    nums = re.findall('(\\d*\\.\\d*\\%)', str)\n",
    "    if (nums):\n",
    "        return float(nums[0].strip('%'))/100\n",
    "    # 'nn.nn'\n",
    "    nums = re.findall('(\\d*\\.\\d*)', str)\n",
    "    if (nums):\n",
    "        return float(nums[0])\n",
    "    # 'nn'\n",
    "    nums = re.findall('(\\d+)', str)    \n",
    "    if (nums):\n",
    "        return float(nums[0])\n",
    "    return 0\n",
    "\n",
    "# kratio\n",
    "def kdratio_get(str):\n",
    "    return round(percentage_get_(str), 4)\n",
    "\n",
    "# assists\n",
    "def assists_get(str):\n",
    "    nums = re.findall(\"\\d+\", str)\n",
    "    if len(nums) == 0:\n",
    "        return '0', '0'\n",
    "    elif len(nums) == 1:\n",
    "        return nums[0], '0'\n",
    "    return nums[0], nums[1]\n",
    "# kills\n",
    "def kills_get(str):\n",
    "    return assists_get(str)\n",
    "\n",
    "# mostly for 'adr'\n",
    "def num_get(str):\n",
    "    nums = re.findall(\"\\d+\", str)\n",
    "    if len(nums):\n",
    "        return nums[0]\n",
    "    return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse player stat.\n",
    "def player_stat(player_tr):\n",
    "#    items = ['player', 'kills', 'assists', 'deaths', 'kdratio', 'kddiff', 'adr', 'fkdiff', 'adr']\n",
    "    player_st = {}\n",
    "    # player\n",
    "    st_player = player_tr.find('td', attrs={\"class\": \"st-player\"})\n",
    "    player_id = get_player_id(st_player.a)\n",
    "    player_st['player_id'] = player_id\n",
    "    player_st['player'] = get_player_name(st_player.a) # st_player.text\n",
    "    # parse 'kills'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-kills\"}).text\n",
    "    player_st['kills'], player_st['hs'] = kills_get(str)\n",
    "    # parse 'assists'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-assists\"}).text\n",
    "    player_st['assists'], player_st['flash_assists'] = assists_get(str)\n",
    "        \n",
    "    # parse 'kdratio', remove '%' to float.\n",
    "    player_st['kdratio'] = kdratio_get(str)\n",
    "    # 'adr'\n",
    "    item = 'adr'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-\" + item}).text\n",
    "    player_st[item] = num_get(str)\n",
    "    # parse all others.\n",
    "    items = ['deaths', 'kddiff', 'fkdiff', 'rating']\n",
    "    for item in items:\n",
    "        player_st[item] = player_tr.find('td', attrs={\"class\": \"st-\" + item}).text\n",
    "\n",
    "    return {player_id : player_st}\n",
    "\n",
    "def team_stat_scrape(st_table):\n",
    "    # \n",
    "    teamname = st_table.find(\"th\", attrs={\"class\": \"st-teamname\"}).text\n",
    "\n",
    "    # team member st.\n",
    "    player_stat_table = {}\n",
    "    for tr in st_table.find(\"tbody\").find_all(\"tr\"):\n",
    "        player_stat_table.update(player_stat(tr))\n",
    "\n",
    "    # build team/teammate map\n",
    "    teammates = {}\n",
    "    for id_ in player_stat_table:\n",
    "        # insert 'teamname'\n",
    "        player_stat_table[id_]['teamname'] = teamname\n",
    "        player = {id_: player_stat_table[id_]['player']}\n",
    "        teammates.update(player)\n",
    "    \n",
    "    # {teamname: {{id: name}, .., {id: name}}\n",
    "    team = {teamname : teammates}\n",
    "    return team, player_stat_table\n",
    "\n",
    "def stats_table_scrape(stats_table):\n",
    "    ### team\n",
    "    teams = []\n",
    "    player_stat_table = {}\n",
    "    # print(stats_table)\n",
    "    for stat_table in stats_table:\n",
    "        team, player_st = team_stat_scrape(stat_table)\n",
    "        teams.append(team)\n",
    "        player_stat_table.update(player_st)\n",
    "\n",
    "    #display(pd.read_json(json.dumps(teams[0]), orient='index'))\n",
    "    #display(pd.read_json(json.dumps(teams[1]), orient='index'))\n",
    "    #display(pd.read_json(json.dumps(player_stat_table), orient='index'))\n",
    "    return teams, player_stat_table\n",
    "\n",
    "def map_match_scrape(contentCol):\n",
    "    mapstat = {}\n",
    "    # mapstatid\n",
    "    stats_table = contentCol.find_all('table', attrs={'class': 'stats-table'})\n",
    "    teams, player_st = stats_table_scrape(stats_table)\n",
    "    mapstat['team'] = teams\n",
    "    mapstat['stat'] = player_st\n",
    "    return mapstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entrance of match scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def match_scrape(brower, match_info, timedelay=0.1):\n",
    "    start_time = time.time()\n",
    "    match_result = {}\n",
    "    # hold all output scrape results.\n",
    "    match_result.update(match_info)\n",
    "    # print(match_result)\n",
    "\n",
    "    # match performace main page\n",
    "    wait_start = time.time()\n",
    "    browser.visit(hltv_url + match_info['href'])\n",
    "    browser.is_element_present_by_css(\"div.contentCol\", wait_time=3)\n",
    "#    time.sleep(timedelay)\n",
    "    wait_time = time.time() - wait_start\n",
    "    contentCol = BeautifulSoup(browser.html, 'html.parser')       \\\n",
    "                 .find('div', attrs={'class': 'contentCol'})\n",
    "\n",
    "    # 'match_id' from 'match-page-link'\n",
    "    match_page_link = contentCol.find('a', attrs={'class': 'match-page-link'})\n",
    "    match_result['match_id'] = re.findall('/(\\d+)/', match_page_link['href'])[0]\n",
    "        \n",
    "    # get all map's 'href'\n",
    "    stats_match_maps = contentCol.find('div', attrs={'class': 'stats-match-maps'})\n",
    "    as_ = []\n",
    "    if stats_match_maps:\n",
    "        as_ = stats_match_maps.find_all('a', href=True)\n",
    "\n",
    "    perf_mapstat = []   \n",
    "    if len(as_) == 0:\n",
    "        # print(f'single map')\n",
    "    \n",
    "        mapstat = {}\n",
    "        mapstat['map'] = match_info['map']\n",
    "        left = contentCol.find('div', attrs={'class': 'team-left'})\n",
    "        right = contentCol.find('div', attrs={'class': 'team-right'})\n",
    "        mapstat['total_rounds'] = int(left.div.text) + int(right.div.text)\n",
    "        if (int(left.div.text) >= int(right.div.text)):\n",
    "            winner = left\n",
    "        else:\n",
    "            winner = right\n",
    "        mapstat['winner'] = get_team_name(winner.a)\n",
    "        mapstat['winner_id'] = get_team_id(winner.a)\n",
    "        mapstat['win_rounds'] = winner.div.text\n",
    "\n",
    "        # mapstatid\n",
    "        mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", match_info['href'])[0]\n",
    "        # scrape all others\n",
    "        mapstat.update(map_match_scrape(contentCol))\n",
    "        perf_mapstat.append(mapstat)\n",
    "    else:\n",
    "        # print(f'multi-map match')\n",
    "        # get all map's 'href', 'score' and 'map'\n",
    "        match_map_results = []\n",
    "        for a in as_:\n",
    "            map_res = {}\n",
    "            map_res['href'] = a['href']\n",
    "            scores = a.find('div', attrs={'class': 'stats-match-map-result-score'}).text\n",
    "            nums = re.findall('\\d+', scores)\n",
    "            map_res['total_rounds'] = int(nums[0]) + int(nums[1])\n",
    "            if (int(nums[0]) >= int(nums[1])):\n",
    "                map_res['win_rounds'] = nums[0]\n",
    "            else:\n",
    "                map_res['win_rounds'] = nums[1]\n",
    "            map_res['map'] = a.find('div', attrs={'class': 'dynamic-map-name-full'}).text\n",
    "\n",
    "            winner = a.find('div', attrs={'class': 'stats-match-map-winner-logo-con'})\n",
    "            # timing issue here !!!!\n",
    "            if (not winner) or (not winner.img):\n",
    "                href = match_info['href']\n",
    "                print(f'href={href}, winner={winner}, re-try...')\n",
    "                map_res['winner_id'] = '0'\n",
    "                map_res['winner'] = 'unknown'\n",
    "            #print(f'winner {winner}')\n",
    "            else:\n",
    "                map_res['winner_id'] = re.findall(\"\\d+\", winner.img['src'])[0]\n",
    "                map_res['winner'] = winner.img['title']\n",
    "            \n",
    "            match_map_results.append(map_res)\n",
    "\n",
    "        # scrape current webpage, save one visit\n",
    "        for map_res in match_map_results:\n",
    "            if map_res['href'] != match_info['href']:\n",
    "                continue\n",
    "            # print(f'current page {map_res[\"href\"]}')    \n",
    "            mapstat = {}\n",
    "            mapstat.update(map_res)\n",
    "            # mapstatid\n",
    "            mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", map_res['href'])[0]\n",
    "            # scrape all others\n",
    "            mapstat.update(map_match_scrape(contentCol))\n",
    "            perf_mapstat.append(mapstat)\n",
    "\n",
    "        # scrape all other webpages.\n",
    "        for map_res in match_map_results:\n",
    "            if map_res['href'] == match_info['href']:\n",
    "                continue\n",
    "            if 'mapstatsid' not in map_res['href']:\n",
    "                continue\n",
    "\n",
    "            # match performace main page\n",
    "            wait_start = time.time()\n",
    "            browser.visit(hltv_url + map_res['href'])\n",
    "            browser.is_element_present_by_css(\"div.contentCol\", wait_time=3)\n",
    "            wait_time += time.time() - wait_start            \n",
    "            contentCol = BeautifulSoup(browser.html, 'html.parser')       \\\n",
    "                             .find('div', attrs={'class': 'contentCol'})                \n",
    "\n",
    "            # print(f\"'visit {map_res['href']}\")            \n",
    "            mapstat = {}\n",
    "            mapstat.update(map_res)\n",
    "            # mapstatid\n",
    "            mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", map_res['href'])[0]\n",
    "            # scrape all others\n",
    "            mapstat.update(map_match_scrape(contentCol))\n",
    "            perf_mapstat.append(mapstat)\n",
    "\n",
    "    match_result['mapstat'] = perf_mapstat\n",
    "    total = time.time() - start_time\n",
    "    # print(f'wait={wait_time}, total={total}')\n",
    "    return match_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert scrape result to pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_padding(df, column_name, pad):\n",
    "    df[column_name] = [pad] * df.shape[0]\n",
    "    return df\n",
    "\n",
    "def match_stat_df_get(match_res):\n",
    "    # define the order.\n",
    "    column_names = ['player_id','player','match_id','date',\n",
    "                'kills', 'hs', 'assists', 'flash_assists', 'kdratio', 'deaths', 'kddiff', 'adr', 'fkdiff']\n",
    "    player_match_df = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    for mapstat in match_res['mapstat']:\n",
    "        player_map_df = pd.read_json(json.dumps(mapstat['stat']), orient='index')\n",
    "        # padding\n",
    "        player_map_df = df_padding(player_map_df, 'match_id', match_res['match_id'])\n",
    "        player_map_df = df_padding(player_map_df, 'winner', mapstat['winner'])\n",
    "        player_map_df = df_padding(player_map_df, 'winner_id', mapstat['winner_id'])\n",
    "        \n",
    "        player_map_df = df_padding(player_map_df, 'total_rounds', mapstat['total_rounds'])\n",
    "        player_map_df = df_padding(player_map_df, 'mapstatid', mapstat['mapstatid'])\n",
    "        \n",
    "        player_map_df = df_padding(player_map_df, 'win_rounds', mapstat['win_rounds'])\n",
    "        player_map_df = df_padding(player_map_df, 'map', mapstat['map'])\n",
    "        \n",
    "        t_str = datetime.utcfromtimestamp(match_res[key_timestamp]).strftime('%Y-%m-%d')\n",
    "        player_map_df = df_padding(player_map_df, 'date', t_str)\n",
    "\n",
    "        player_match_df = pd.concat([player_match_df, player_map_df])\n",
    "    return player_match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_column_names = [\n",
    "     'player_id',\n",
    "     'player',\n",
    "     'match_id',\n",
    "     'date',\n",
    "     'kills',\n",
    "     'hs',\n",
    "     'assists',\n",
    "     'flash_assists',\n",
    "     'kdratio',\n",
    "     'deaths',\n",
    "     'kddiff',\n",
    "     'adr',\n",
    "     'fkdiff',\n",
    "     'rating',\n",
    "     'teamname',\n",
    "     'winner',\n",
    "     'winner_id',\n",
    "     'total_rounds',\n",
    "     'mapstatid',\n",
    "     'win_rounds',\n",
    "     'map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_scrape(browser, match_urls):\n",
    "    if len(match_urls) == 0:\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 1st url's time pending to the output filename\n",
    "    ts = match_urls[0]['ts']\n",
    "    t1 = datetime.utcfromtimestamp(ts).strftime('%Y%m%d')\n",
    "    # last url's time pending to the output filename\n",
    "    ts = match_urls[-1]['ts']\n",
    "    t2 = datetime.utcfromtimestamp(ts).strftime('%Y%m%d')\n",
    "    output_filename = 'player_mapstat_' + t1 + '_' + t2 + '.csv'\n",
    "\n",
    "    # create an empty file\n",
    "    df = pd.DataFrame(columns=output_df_column_names)\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    # append the file.\n",
    "    count = 0\n",
    "    for match_result in match_urls:\n",
    "        res = match_scrape(browser, match_result)\n",
    "        df = match_stat_df_get(res)\n",
    "        df[output_df_column_names].to_csv(output_filename, mode='a', header=False)\n",
    "        count += 1\n",
    "        if ((count % 10) == 0):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'scraped {count} matches in {elapsed_time} seconds.')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Done. scraped {count} matches in {elapsed_time} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\n",
    "#    '2019/1/1',\n",
    "#    '2018/12/15',\n",
    "#    '2018/12/1',\n",
    "#    '2018/11/15',\n",
    "#    '2018/11/1',\n",
    "#    '2018/10/15',\n",
    "#    '2018/10/1',\n",
    "#    '2018/9/15',\n",
    "#    '2018/9/1',\n",
    "#    '2018/8/15',\n",
    "#    '2018/8/1',\n",
    "    '2018/7/15',\n",
    "    '2018/7/1',\n",
    "    '2018/6/15',\n",
    "    '2018/6/1'\n",
    "]\n",
    "#next_url = '/stats/matches'\n",
    "#next_url = '/stats/matches?offset=21400'\n",
    "next_url = '/stats/matches?offset=29950'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now 06/20/2020, 20:25:09\n",
      "Included 1st match timestamp: 2018/07/15 02:05:00\n",
      "Excluded 1st match timestamp: 2018/07/01 03:10:00\n",
      "total 514, next_url=/stats/matches?offset=30450\n",
      "\n",
      "\n",
      "scraped 10 matches in 24.120733499526978 seconds.\n",
      "scraped 20 matches in 47.39499759674072 seconds.\n",
      "scraped 30 matches in 75.5169665813446 seconds.\n",
      "scraped 40 matches in 100.56453585624695 seconds.\n",
      "scraped 50 matches in 128.20525479316711 seconds.\n",
      "scraped 60 matches in 149.5764560699463 seconds.\n",
      "scraped 70 matches in 165.69072818756104 seconds.\n",
      "scraped 80 matches in 181.89233422279358 seconds.\n",
      "scraped 90 matches in 196.2072446346283 seconds.\n",
      "scraped 100 matches in 218.707421541214 seconds.\n",
      "scraped 110 matches in 234.807767868042 seconds.\n",
      "scraped 120 matches in 254.14133262634277 seconds.\n",
      "scraped 130 matches in 270.33500838279724 seconds.\n",
      "scraped 140 matches in 291.6768183708191 seconds.\n",
      "scraped 150 matches in 307.3658483028412 seconds.\n",
      "scraped 160 matches in 336.7264440059662 seconds.\n",
      "scraped 170 matches in 357.5587890148163 seconds.\n",
      "scraped 180 matches in 378.97699642181396 seconds.\n",
      "scraped 190 matches in 406.74906849861145 seconds.\n",
      "scraped 200 matches in 428.63240671157837 seconds.\n",
      "scraped 210 matches in 452.7408175468445 seconds.\n",
      "scraped 220 matches in 475.5881519317627 seconds.\n",
      "scraped 230 matches in 498.1458730697632 seconds.\n",
      "scraped 240 matches in 528.5396392345428 seconds.\n",
      "scraped 250 matches in 547.7247424125671 seconds.\n",
      "scraped 260 matches in 568.6978459358215 seconds.\n",
      "scraped 270 matches in 592.914363861084 seconds.\n",
      "scraped 280 matches in 618.6037871837616 seconds.\n",
      "scraped 290 matches in 639.6239936351776 seconds.\n",
      "scraped 300 matches in 668.4741208553314 seconds.\n",
      "scraped 310 matches in 693.1042857170105 seconds.\n",
      "scraped 320 matches in 716.2053492069244 seconds.\n",
      "scraped 330 matches in 740.5272126197815 seconds.\n",
      "scraped 340 matches in 757.7612693309784 seconds.\n",
      "scraped 350 matches in 779.3465592861176 seconds.\n",
      "scraped 360 matches in 810.0318722724915 seconds.\n",
      "scraped 370 matches in 824.9900343418121 seconds.\n",
      "scraped 380 matches in 846.3619923591614 seconds.\n",
      "scraped 390 matches in 864.3442661762238 seconds.\n",
      "href=/stats/matches/mapstatsid/69913/winstrike-vs-nexus, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "href=/stats/matches/mapstatsid/69912/winstrike-vs-nexus, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 400 matches in 887.4178564548492 seconds.\n",
      "scraped 410 matches in 906.5934615135193 seconds.\n",
      "scraped 420 matches in 924.5368511676788 seconds.\n",
      "scraped 430 matches in 937.1960852146149 seconds.\n",
      "scraped 440 matches in 964.8361620903015 seconds.\n",
      "scraped 450 matches in 992.6056845188141 seconds.\n",
      "scraped 460 matches in 1021.4356706142426 seconds.\n",
      "scraped 470 matches in 1047.6492471694946 seconds.\n",
      "scraped 480 matches in 1080.4076981544495 seconds.\n",
      "scraped 490 matches in 1115.2750401496887 seconds.\n",
      "scraped 500 matches in 1142.3961596488953 seconds.\n",
      "scraped 510 matches in 1164.154666185379 seconds.\n",
      "Done. scraped 514 matches in 1174.960143327713 seconds.\n",
      "now 06/20/2020, 20:45:01\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2018/07/01 16:10:00\n",
      "Excluded 1st match timestamp: 2018/06/15 06:40:00\n",
      "total 953, next_url=/stats/matches?offset=31400\n",
      "\n",
      "\n",
      "scraped 10 matches in 33.952550411224365 seconds.\n",
      "scraped 20 matches in 67.74020552635193 seconds.\n",
      "scraped 30 matches in 92.56586623191833 seconds.\n",
      "scraped 40 matches in 118.61095142364502 seconds.\n",
      "scraped 50 matches in 150.3576443195343 seconds.\n",
      "scraped 60 matches in 183.76495695114136 seconds.\n",
      "scraped 70 matches in 220.29060220718384 seconds.\n",
      "scraped 80 matches in 256.55567240715027 seconds.\n",
      "scraped 90 matches in 287.68595576286316 seconds.\n",
      "scraped 100 matches in 320.9614543914795 seconds.\n",
      "scraped 110 matches in 358.7982769012451 seconds.\n",
      "scraped 120 matches in 387.79242396354675 seconds.\n",
      "scraped 130 matches in 426.1670277118683 seconds.\n",
      "scraped 140 matches in 456.6763846874237 seconds.\n",
      "scraped 150 matches in 483.0373737812042 seconds.\n",
      "scraped 160 matches in 498.68054938316345 seconds.\n",
      "scraped 170 matches in 521.8559105396271 seconds.\n",
      "scraped 180 matches in 547.5908536911011 seconds.\n",
      "scraped 190 matches in 576.4521021842957 seconds.\n",
      "scraped 200 matches in 602.3855724334717 seconds.\n",
      "scraped 210 matches in 627.0662333965302 seconds.\n",
      "scraped 220 matches in 667.7865614891052 seconds.\n",
      "scraped 230 matches in 705.1593234539032 seconds.\n",
      "scraped 240 matches in 729.13356590271 seconds.\n",
      "scraped 250 matches in 758.16588306427 seconds.\n",
      "scraped 260 matches in 798.5732328891754 seconds.\n",
      "scraped 270 matches in 831.0441279411316 seconds.\n",
      "scraped 280 matches in 853.4842655658722 seconds.\n",
      "scraped 290 matches in 876.8626792430878 seconds.\n",
      "href=/stats/matches/mapstatsid/69467/x-kom-vs-nexus, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "href=/stats/matches/mapstatsid/69466/x-kom-vs-nexus, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 300 matches in 909.604376077652 seconds.\n",
      "scraped 310 matches in 922.767804145813 seconds.\n",
      "scraped 320 matches in 946.1838400363922 seconds.\n",
      "scraped 330 matches in 993.779604434967 seconds.\n",
      "scraped 340 matches in 1030.5191824436188 seconds.\n",
      "scraped 350 matches in 1058.0522904396057 seconds.\n",
      "scraped 360 matches in 1093.6720769405365 seconds.\n",
      "scraped 370 matches in 1117.8072617053986 seconds.\n",
      "scraped 380 matches in 1148.264410018921 seconds.\n",
      "scraped 390 matches in 1177.5371158123016 seconds.\n",
      "scraped 400 matches in 1207.3373346328735 seconds.\n",
      "scraped 410 matches in 1231.402706861496 seconds.\n",
      "scraped 420 matches in 1272.9457001686096 seconds.\n",
      "scraped 430 matches in 1300.9729685783386 seconds.\n",
      "scraped 440 matches in 1338.2968361377716 seconds.\n",
      "scraped 450 matches in 1384.0014379024506 seconds.\n",
      "scraped 460 matches in 1428.6387135982513 seconds.\n",
      "scraped 470 matches in 1461.8283467292786 seconds.\n",
      "scraped 480 matches in 1500.4732472896576 seconds.\n",
      "scraped 490 matches in 1538.070927619934 seconds.\n",
      "scraped 500 matches in 1583.5209565162659 seconds.\n",
      "scraped 510 matches in 1621.4690580368042 seconds.\n",
      "scraped 520 matches in 1640.8196160793304 seconds.\n",
      "scraped 530 matches in 1672.5287609100342 seconds.\n",
      "scraped 540 matches in 1707.5013508796692 seconds.\n",
      "scraped 550 matches in 1751.4843728542328 seconds.\n",
      "scraped 560 matches in 1791.713229417801 seconds.\n",
      "scraped 570 matches in 1830.4258489608765 seconds.\n",
      "scraped 580 matches in 1857.3566355705261 seconds.\n",
      "scraped 590 matches in 1877.0304522514343 seconds.\n",
      "scraped 600 matches in 1906.609601020813 seconds.\n",
      "scraped 610 matches in 1927.8010437488556 seconds.\n",
      "scraped 620 matches in 1943.135891675949 seconds.\n",
      "scraped 630 matches in 1969.6191256046295 seconds.\n",
      "scraped 640 matches in 1996.9563937187195 seconds.\n",
      "scraped 650 matches in 2012.330485343933 seconds.\n",
      "scraped 660 matches in 2027.700991153717 seconds.\n",
      "scraped 670 matches in 2053.4357211589813 seconds.\n",
      "scraped 680 matches in 2086.5490729808807 seconds.\n",
      "scraped 690 matches in 2101.2857077121735 seconds.\n",
      "scraped 700 matches in 2121.398060798645 seconds.\n",
      "scraped 710 matches in 2147.3286147117615 seconds.\n",
      "scraped 720 matches in 2180.1613850593567 seconds.\n",
      "scraped 730 matches in 2210.718067407608 seconds.\n",
      "scraped 740 matches in 2237.5876524448395 seconds.\n",
      "scraped 750 matches in 2254.6580834388733 seconds.\n",
      "scraped 760 matches in 2275.010875940323 seconds.\n",
      "scraped 770 matches in 2305.646641969681 seconds.\n",
      "scraped 780 matches in 2334.1695413589478 seconds.\n",
      "scraped 790 matches in 2366.6868884563446 seconds.\n",
      "scraped 800 matches in 2415.5795423984528 seconds.\n",
      "scraped 810 matches in 2449.5409338474274 seconds.\n",
      "scraped 820 matches in 2480.3868701457977 seconds.\n",
      "scraped 830 matches in 2521.378094434738 seconds.\n",
      "scraped 840 matches in 2557.2252831459045 seconds.\n",
      "scraped 850 matches in 2575.0186953544617 seconds.\n",
      "scraped 860 matches in 2607.136858701706 seconds.\n",
      "scraped 870 matches in 2648.9815623760223 seconds.\n",
      "scraped 880 matches in 2687.2169358730316 seconds.\n",
      "href=/stats/matches/mapstatsid/68812/alternate-attax-vs-pact, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "href=/stats/matches/mapstatsid/68811/alternate-attax-vs-pact, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 890 matches in 2718.545858860016 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "href=/stats/matches/mapstatsid/68797/valiance-vs-illuminar, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "href=/stats/matches/mapstatsid/68796/valiance-vs-illuminar, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 900 matches in 2744.350566148758 seconds.\n",
      "scraped 910 matches in 2775.3084688186646 seconds.\n",
      "scraped 920 matches in 2806.584896326065 seconds.\n",
      "scraped 930 matches in 2831.453374862671 seconds.\n",
      "scraped 940 matches in 2855.5208609104156 seconds.\n",
      "scraped 950 matches in 2886.32293176651 seconds.\n",
      "Done. scraped 953 matches in 2891.3518726825714 seconds.\n",
      "now 06/20/2020, 21:33:48\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2018/06/15 08:45:00\n",
      "Excluded 1st match timestamp: 2018/06/01 02:30:00\n",
      "total 739, next_url=/stats/matches?offset=32100\n",
      "\n",
      "\n",
      "scraped 10 matches in 24.589640855789185 seconds.\n",
      "scraped 20 matches in 45.99780464172363 seconds.\n",
      "href=/stats/matches/mapstatsid/68709/winstrike-vs-spirit, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 30 matches in 69.58216595649719 seconds.\n",
      "href=/stats/matches/mapstatsid/68707/spirit-vs-winstrike, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 40 matches in 94.31110095977783 seconds.\n",
      "scraped 50 matches in 118.12929129600525 seconds.\n",
      "scraped 60 matches in 148.6046323776245 seconds.\n",
      "scraped 70 matches in 170.03901553153992 seconds.\n",
      "scraped 80 matches in 194.41307044029236 seconds.\n",
      "scraped 90 matches in 217.47894382476807 seconds.\n",
      "scraped 100 matches in 242.3569052219391 seconds.\n",
      "scraped 110 matches in 264.827232837677 seconds.\n",
      "scraped 120 matches in 289.27987575531006 seconds.\n",
      "scraped 130 matches in 319.4631311893463 seconds.\n",
      "scraped 140 matches in 356.9463560581207 seconds.\n",
      "href=/stats/matches/mapstatsid/68568/x-kom-vs-epsilon, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 150 matches in 386.87495827674866 seconds.\n",
      "href=/stats/matches/mapstatsid/68567/x-kom-vs-epsilon, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 160 matches in 404.8409700393677 seconds.\n",
      "scraped 170 matches in 442.56789088249207 seconds.\n",
      "scraped 180 matches in 464.7494204044342 seconds.\n",
      "scraped 190 matches in 493.22755455970764 seconds.\n",
      "scraped 200 matches in 522.2183134555817 seconds.\n",
      "scraped 210 matches in 541.5531122684479 seconds.\n",
      "scraped 220 matches in 572.6693775653839 seconds.\n",
      "scraped 230 matches in 606.5305984020233 seconds.\n",
      "scraped 240 matches in 627.4921810626984 seconds.\n",
      "scraped 250 matches in 668.6012029647827 seconds.\n",
      "scraped 260 matches in 710.183354139328 seconds.\n",
      "scraped 270 matches in 738.8090109825134 seconds.\n",
      "scraped 280 matches in 772.4804372787476 seconds.\n",
      "scraped 290 matches in 814.9479353427887 seconds.\n",
      "scraped 300 matches in 850.4535920619965 seconds.\n",
      "scraped 310 matches in 889.062194108963 seconds.\n",
      "scraped 320 matches in 920.3424282073975 seconds.\n",
      "scraped 330 matches in 953.6083748340607 seconds.\n",
      "scraped 340 matches in 983.9885132312775 seconds.\n",
      "scraped 350 matches in 1013.2354118824005 seconds.\n",
      "scraped 360 matches in 1041.6917550563812 seconds.\n",
      "scraped 370 matches in 1059.9369671344757 seconds.\n",
      "scraped 380 matches in 1076.4425041675568 seconds.\n",
      "href=/stats/matches/mapstatsid/68308/x-kom-vs-pompa, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "href=/stats/matches/mapstatsid/68303/x-kom-vs-pompa, winner=<div class=\"stats-match-map-winner-logo-con\"></div>, re-try...\n",
      "scraped 390 matches in 1106.3775129318237 seconds.\n",
      "scraped 400 matches in 1131.759830713272 seconds.\n",
      "scraped 410 matches in 1155.5751085281372 seconds.\n",
      "scraped 420 matches in 1185.7113044261932 seconds.\n",
      "scraped 430 matches in 1206.9918553829193 seconds.\n",
      "scraped 440 matches in 1246.4858767986298 seconds.\n",
      "scraped 450 matches in 1275.5179421901703 seconds.\n",
      "scraped 460 matches in 1311.459255695343 seconds.\n",
      "scraped 470 matches in 1339.5079679489136 seconds.\n",
      "scraped 480 matches in 1376.5040121078491 seconds.\n",
      "scraped 490 matches in 1409.2602488994598 seconds.\n",
      "scraped 500 matches in 1445.657383441925 seconds.\n",
      "scraped 510 matches in 1487.8559577465057 seconds.\n",
      "scraped 520 matches in 1511.7499079704285 seconds.\n",
      "scraped 530 matches in 1541.6366696357727 seconds.\n",
      "scraped 540 matches in 1576.1225743293762 seconds.\n",
      "scraped 550 matches in 1609.634021282196 seconds.\n",
      "scraped 560 matches in 1643.346441268921 seconds.\n",
      "scraped 570 matches in 1683.2784025669098 seconds.\n",
      "scraped 580 matches in 1722.9550740718842 seconds.\n",
      "scraped 590 matches in 1766.602012872696 seconds.\n",
      "scraped 600 matches in 1804.0412390232086 seconds.\n",
      "scraped 610 matches in 1846.3300938606262 seconds.\n",
      "scraped 620 matches in 1877.8910870552063 seconds.\n",
      "scraped 630 matches in 1926.4946417808533 seconds.\n",
      "scraped 640 matches in 1966.3354806900024 seconds.\n",
      "scraped 650 matches in 2009.2450776100159 seconds.\n",
      "scraped 660 matches in 2053.556398868561 seconds.\n",
      "scraped 670 matches in 2098.8643662929535 seconds.\n",
      "scraped 680 matches in 2136.56058549881 seconds.\n",
      "scraped 690 matches in 2173.9640765190125 seconds.\n",
      "scraped 700 matches in 2210.896641969681 seconds.\n",
      "scraped 710 matches in 2233.135930299759 seconds.\n",
      "scraped 720 matches in 2283.7572717666626 seconds.\n",
      "scraped 730 matches in 2315.952196121216 seconds.\n",
      "Done. scraped 739 matches in 2350.628892183304 seconds.\n",
      "now 06/20/2020, 22:13:27\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current date and time\n",
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print(f\"now {now_str}\")\n",
    "\n",
    "start_date = None\n",
    "for stop_date in dates:\n",
    "    if not start_date:\n",
    "        start_date = stop_date\n",
    "        continue\n",
    "    start_ts = datetime.timestamp(datetime.strptime(start_date, \"%Y/%m/%d\"))\n",
    "    stop_ts = datetime.timestamp(datetime.strptime(stop_date, \"%Y/%m/%d\"))\n",
    "    \n",
    "    match_urls, next_url = match_list_get(browser, start_ts, stop_ts, next_url)\n",
    "    print(f'total {len(match_urls)}, next_url={next_url}\\n\\n')\n",
    "\n",
    "    match_list_scrape(browser, match_urls)\n",
    "\n",
    "    # current date and time\n",
    "    now = datetime.now()\n",
    "    now_str = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    print(f\"now {now_str}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
