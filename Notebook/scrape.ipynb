{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a filename (with path) such that it is accessible.\n",
    "def get_fullname(filename):\n",
    "    # as long as it is accessible.\n",
    "    if os.path.isfile(filename):\n",
    "        return filename\n",
    "\n",
    "    # search current working directory.\n",
    "    cur_working_dir = os.getcwd()\n",
    "    for i in range(50):\n",
    "        for (path, dir, files) in os.walk(cur_working_dir):\n",
    "            if filename in files:\n",
    "                return os.path.join(path, filename)\n",
    "\n",
    "        # check parent directory\n",
    "        parent = os.path.dirname(cur_working_dir)\n",
    "        if cur_working_dir == parent:\n",
    "            break;\n",
    "        cur_working_dir = parent\n",
    "\n",
    "    # did not found, simply return.\n",
    "    print(f\"file {filename} not found !!!\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Initiate headless driver for deployment\n",
    "browser = Browser(\"chrome\", executable_path=get_fullname(\"chromedriver.exe\"), headless=True)\n",
    "#browser = Browser('chrome', executable_path=get_fullname(\"chromedriver.exe\"))\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url\n",
    "hltv_url = 'https://www.hltv.org'\n",
    "output_filename = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_perf_table = 'perf_table'\n",
    "key_AWP_kills = 'AWP_kills'\n",
    "## top level\n",
    "key_timestamp = 'ts'\n",
    "key_map_stats = 'map_stats'\n",
    "key_teams = 'teams'\n",
    "key_player_st = 'match_player_st'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visist '/stats/matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# href=\"/stats/teams/4471/bemyfRAG\"\n",
    "# href=\"/stats/players/15090/PwnAlone\"\n",
    "def get_team_id(a):\n",
    "    sl = a['href'].split('/')\n",
    "    return sl[3]\n",
    "get_player_id = get_team_id\n",
    "\n",
    "def get_team_name(a):\n",
    "    sl = a['href'].split('/')\n",
    "    return sl[4]\n",
    "get_player_name = get_team_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only timestamp, map (if single map) and href are used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_get(browser, start, stop, next_url = '/stats/matches'):\n",
    "    # start is the newer match data, larger value\n",
    "    if start < stop:\n",
    "        t = stop\n",
    "        stop = start\n",
    "        start = t\n",
    "\n",
    "    start_url = False\n",
    "\n",
    "    # Visit the site to Scrape\n",
    "    ts = 0\n",
    "    all_done = False\n",
    "    match_urls = []\n",
    "\n",
    "    for cnt in range(10000):\n",
    "        # Visit the site to Scrape\n",
    "        browser.visit(hltv_url + next_url)\n",
    "        browser.is_element_present_by_tag('div.contentCol', wait_time=3)\n",
    "        content = BeautifulSoup(browser.html, 'html.parser').find('div', attrs={\"class\": \"contentCol\"})\n",
    "\n",
    "        # all matches in this page\n",
    "        trs = content.tbody.find_all('tr')\n",
    "\n",
    "        for tr in trs:\n",
    "            match_info= {}\n",
    "\n",
    "            # data-unix\n",
    "            td = tr.find('td', attrs={'class': 'date-col'})\n",
    "            t = int(td.div['data-unix'])/1000\n",
    "            if (t > start):\n",
    "                continue\n",
    "            if (t <= stop):\n",
    "                all_done = True\n",
    "                str = datetime.utcfromtimestamp(t).strftime('%Y/%m/%d %H:%M:%S')\n",
    "                print(f'Excluded 1st match timestamp: {str}')\n",
    "                break\n",
    "\n",
    "            if not start_url:\n",
    "                start_url = True\n",
    "                str = datetime.utcfromtimestamp(t).strftime('%Y/%m/%d %H:%M:%S')\n",
    "                print(f'Included 1st match timestamp: {str}')\n",
    "            match_info['href'] = td.a['href']\n",
    "            match_info['ts'] = t\n",
    "            # map\n",
    "            match_info['map'] = tr.find('div', attrs={'class': 'dynamic-map-name-full'}).text\n",
    "            match_urls.append(match_info)\n",
    "\n",
    "        # keep current page\n",
    "        # if in middle of the page, may overlapping, prevent missing matches\n",
    "        if all_done:\n",
    "            break\n",
    "        # ========================================================================\n",
    "        # next page\n",
    "        # ========================================================================\n",
    "        # next page\n",
    "        a = content.find('a', attrs={\"class\": \"pagination-next\"}, href=True)\n",
    "        if not a:\n",
    "            print(f'Done {len(match_urls)} matches scraping.')\n",
    "            break\n",
    "        next_url = a['href']\n",
    "\n",
    "        # only during development\n",
    "        if cnt == 1000:\n",
    "            print(f'Truncate the match lists !!!!!')\n",
    "            print(f'Change next_url from: {next_url}')        \n",
    "            next_url = '/stats/matches?offset=79700'\n",
    "            print(f'                  to: {next_url}')  \n",
    "    \n",
    "    # return all urls and next_page_url\n",
    "    return match_urls, next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### player stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_get_(str):\n",
    "    # 'nn.nn%'\n",
    "    nums = re.findall('(\\d*\\.\\d*\\%)', str)\n",
    "    if (nums):\n",
    "        return float(nums[0].strip('%'))/100\n",
    "    # 'nn.nn'\n",
    "    nums = re.findall('(\\d*\\.\\d*)', str)\n",
    "    if (nums):\n",
    "        return float(nums[0])\n",
    "    # 'nn'\n",
    "    nums = re.findall('(\\d+)', str)    \n",
    "    if (nums):\n",
    "        return float(nums[0])\n",
    "    return 0\n",
    "\n",
    "# kratio\n",
    "def kdratio_get(str):\n",
    "    return round(percentage_get_(str), 4)\n",
    "\n",
    "# assists\n",
    "def assists_get(str):\n",
    "    nums = re.findall(\"\\d+\", str)\n",
    "    if len(nums) == 0:\n",
    "        return '0', '0'\n",
    "    elif len(nums) == 1:\n",
    "        return nums[0], '0'\n",
    "    return nums[0], nums[1]\n",
    "# kills\n",
    "def kills_get(str):\n",
    "    return assists_get(str)\n",
    "\n",
    "# mostly for 'adr'\n",
    "def num_get(str):\n",
    "    nums = re.findall(\"\\d+\", str)\n",
    "    if len(nums):\n",
    "        return nums[0]\n",
    "    return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse player stat.\n",
    "def player_stat(player_tr):\n",
    "#    items = ['player', 'kills', 'assists', 'deaths', 'kdratio', 'kddiff', 'adr', 'fkdiff', 'adr']\n",
    "    player_st = {}\n",
    "    # player\n",
    "    st_player = player_tr.find('td', attrs={\"class\": \"st-player\"})\n",
    "    player_id = get_player_id(st_player.a)\n",
    "    player_st['player_id'] = player_id\n",
    "    player_st['player'] = get_player_name(st_player.a) # st_player.text\n",
    "    # parse 'kills'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-kills\"}).text\n",
    "    player_st['kills'], player_st['hs'] = kills_get(str)\n",
    "    # parse 'assists'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-assists\"}).text\n",
    "    player_st['assists'], player_st['flash_assists'] = assists_get(str)\n",
    "        \n",
    "    # parse 'kdratio', remove '%' to float.\n",
    "    player_st['kdratio'] = kdratio_get(str)\n",
    "    # 'adr'\n",
    "    item = 'adr'\n",
    "    str = player_tr.find('td', attrs={\"class\": \"st-\" + item}).text\n",
    "    player_st[item] = num_get(str)\n",
    "    # parse all others.\n",
    "    items = ['deaths', 'kddiff', 'fkdiff', 'rating']\n",
    "    for item in items:\n",
    "        player_st[item] = player_tr.find('td', attrs={\"class\": \"st-\" + item}).text\n",
    "\n",
    "    return {player_id : player_st}\n",
    "\n",
    "def team_stat_scrape(st_table):\n",
    "    # \n",
    "    teamname = st_table.find(\"th\", attrs={\"class\": \"st-teamname\"}).text\n",
    "\n",
    "    # team member st.\n",
    "    player_stat_table = {}\n",
    "    for tr in st_table.find(\"tbody\").find_all(\"tr\"):\n",
    "        player_stat_table.update(player_stat(tr))\n",
    "\n",
    "    # build team/teammate map\n",
    "    teammates = {}\n",
    "    for id_ in player_stat_table:\n",
    "        # insert 'teamname'\n",
    "        player_stat_table[id_]['teamname'] = teamname\n",
    "        player = {id_: player_stat_table[id_]['player']}\n",
    "        teammates.update(player)\n",
    "    \n",
    "    # {teamname: {{id: name}, .., {id: name}}\n",
    "    team = {teamname : teammates}\n",
    "    return team, player_stat_table\n",
    "\n",
    "def stats_table_scrape(stats_table):\n",
    "    ### team\n",
    "    teams = []\n",
    "    player_stat_table = {}\n",
    "    # print(stats_table)\n",
    "    for stat_table in stats_table:\n",
    "        team, player_st = team_stat_scrape(stat_table)\n",
    "        teams.append(team)\n",
    "        player_stat_table.update(player_st)\n",
    "\n",
    "    #display(pd.read_json(json.dumps(teams[0]), orient='index'))\n",
    "    #display(pd.read_json(json.dumps(teams[1]), orient='index'))\n",
    "    #display(pd.read_json(json.dumps(player_stat_table), orient='index'))\n",
    "    return teams, player_stat_table\n",
    "\n",
    "def map_match_scrape(contentCol):\n",
    "    mapstat = {}\n",
    "    # mapstatid\n",
    "    stats_table = contentCol.find_all('table', attrs={'class': 'stats-table'})\n",
    "    teams, player_st = stats_table_scrape(stats_table)\n",
    "    mapstat['team'] = teams\n",
    "    mapstat['stat'] = player_st\n",
    "    return mapstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entrance of match scrape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def match_scrape(brower, match_info, timedelay=0.1):\n",
    "    start_time = time.time()\n",
    "    match_result = {}\n",
    "    # hold all output scrape results.\n",
    "    match_result.update(match_info)\n",
    "    # print(match_result)\n",
    "\n",
    "    # match performace main page\n",
    "    wait_start = time.time()\n",
    "    browser.visit(hltv_url + match_info['href'])\n",
    "    browser.is_element_present_by_css(\"div.contentCol\", wait_time=3)\n",
    "#    time.sleep(timedelay)\n",
    "    wait_time = time.time() - wait_start\n",
    "    contentCol = BeautifulSoup(browser.html, 'html.parser')       \\\n",
    "                 .find('div', attrs={'class': 'contentCol'})\n",
    "\n",
    "    # 'match_id' from 'match-page-link'\n",
    "    match_page_link = contentCol.find('a', attrs={'class': 'match-page-link'})\n",
    "    match_result['match_id'] = re.findall('/(\\d+)/', match_page_link['href'])[0]\n",
    "        \n",
    "    # get all map's 'href'\n",
    "    stats_match_maps = contentCol.find('div', attrs={'class': 'stats-match-maps'})\n",
    "    as_ = []\n",
    "    if stats_match_maps:\n",
    "        as_ = stats_match_maps.find_all('a', href=True)\n",
    "\n",
    "    perf_mapstat = []   \n",
    "    if len(as_) == 0:\n",
    "        # print(f'single map')\n",
    "    \n",
    "        mapstat = {}\n",
    "        mapstat['map'] = match_info['map']\n",
    "        left = contentCol.find('div', attrs={'class': 'team-left'})\n",
    "        right = contentCol.find('div', attrs={'class': 'team-right'})\n",
    "        mapstat['total_rounds'] = int(left.div.text) + int(right.div.text)\n",
    "        if (int(left.div.text) >= int(right.div.text)):\n",
    "            winner = left\n",
    "        else:\n",
    "            winner = right\n",
    "        mapstat['winner'] = get_team_name(winner.a)\n",
    "        mapstat['winner_id'] = get_team_id(winner.a)\n",
    "        mapstat['win_rounds'] = winner.div.text\n",
    "\n",
    "        # mapstatid\n",
    "        mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", match_info['href'])[0]\n",
    "        # scrape all others\n",
    "        mapstat.update(map_match_scrape(contentCol))\n",
    "        perf_mapstat.append(mapstat)\n",
    "    else:\n",
    "        # print(f'multi-map match')\n",
    "        # get all map's 'href', 'score' and 'map'\n",
    "        match_map_results = []\n",
    "        for a in as_:\n",
    "            map_res = {}\n",
    "            map_res['href'] = a['href']\n",
    "            scores = a.find('div', attrs={'class': 'stats-match-map-result-score'}).text\n",
    "            nums = re.findall('\\d+', scores)\n",
    "            map_res['total_rounds'] = int(nums[0]) + int(nums[1])\n",
    "            if (int(nums[0]) >= int(nums[1])):\n",
    "                map_res['win_rounds'] = nums[0]\n",
    "            else:\n",
    "                map_res['win_rounds'] = nums[1]\n",
    "            map_res['map'] = a.find('div', attrs={'class': 'dynamic-map-name-full'}).text\n",
    "\n",
    "            winner = a.find('div', attrs={'class': 'stats-match-map-winner-logo-con'})\n",
    "            # timing issue here !!!!\n",
    "            if (not winner) or (not winner.img):\n",
    "                href = match_info['href']\n",
    "                print(f'href={href}, winner={winner}, re-try...')\n",
    "                map_res['winner_id'] = '0'\n",
    "                map_res['winner'] = 'unknown'\n",
    "            #print(f'winner {winner}')\n",
    "            else:\n",
    "                map_res['winner_id'] = re.findall(\"\\d+\", winner.img['src'])[0]\n",
    "                map_res['winner'] = winner.img['title']\n",
    "            \n",
    "            match_map_results.append(map_res)\n",
    "\n",
    "        # scrape current webpage, save one visit\n",
    "        for map_res in match_map_results:\n",
    "            if map_res['href'] != match_info['href']:\n",
    "                continue\n",
    "            # print(f'current page {map_res[\"href\"]}')    \n",
    "            mapstat = {}\n",
    "            mapstat.update(map_res)\n",
    "            # mapstatid\n",
    "            mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", map_res['href'])[0]\n",
    "            # scrape all others\n",
    "            mapstat.update(map_match_scrape(contentCol))\n",
    "            perf_mapstat.append(mapstat)\n",
    "\n",
    "        # scrape all other webpages.\n",
    "        for map_res in match_map_results:\n",
    "                continue\n",
    "            if map_res['href'] == match_info['href']:\n",
    "                continue\n",
    "\n",
    "            # match performace main page\n",
    "            wait_start = time.time()\n",
    "            browser.visit(hltv_url + map_res['href'])\n",
    "            browser.is_element_present_by_css(\"div.contentCol\", wait_time=3)\n",
    "            wait_time += time.time() - wait_start            \n",
    "            contentCol = BeautifulSoup(browser.html, 'html.parser')       \\\n",
    "                             .find('div', attrs={'class': 'contentCol'})                \n",
    "\n",
    "            # print(f\"'visit {map_res['href']}\")            \n",
    "            mapstat = {}\n",
    "            mapstat.update(map_res)\n",
    "            # mapstatid\n",
    "            # mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", map_res['href'])[0]\n",
    "            mapstat['mapstatid'] = mapstatsid_get(match_info['href'])[0]\n",
    "            mapstatsid_pend(mapstat['mapstatid'])\n",
    "            # scrape all others\n",
    "            mapstat.update(map_match_scrape(contentCol))\n",
    "            perf_mapstat.append(mapstat)\n",
    "\n",
    "    match_result['mapstat'] = perf_mapstat\n",
    "    total = time.time() - start_time\n",
    "    # print(f'wait={wait_time}, total={total}')\n",
    "    return match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_scrape(brower, match_info, timedelay=0.1):\n",
    "    start_time = time.time()\n",
    "    match_result = {}\n",
    "\n",
    "    # match performace main page\n",
    "    wait_start = time.time()\n",
    "    browser.visit(hltv_url + match_info['href'])\n",
    "    rdy = browser.is_element_present_by_css(\"div.contentCol\", wait_time=3)\n",
    "    if rdy == False:\n",
    "        href = match_info['href']\n",
    "        print(f'href={href}, 3 sec timeout, skip it...')\n",
    "        return match_result\n",
    "\n",
    "    # hold all output scrape results.\n",
    "    match_result.update(match_info)\n",
    "    # print(match_result)    \n",
    "\n",
    "    wait_time = time.time() - wait_start\n",
    "    contentCol = BeautifulSoup(browser.html, 'html.parser')       \\\n",
    "                 .find('div', attrs={'class': 'contentCol'})\n",
    "\n",
    "    # 'match_id' from 'match-page-link'\n",
    "    try:\n",
    "        match_page_link = contentCol.find('a', attrs={'class': 'match-page-link'})\n",
    "        match_result['match_id'] = re.findall('/(\\d+)/', match_page_link['href'])[0]\n",
    "    except:\n",
    "        href = match_info['href']\n",
    "        print(f'exception: href={href}')\n",
    "        print(f'   match_page_link={match_page_link}')\n",
    "        return {}\n",
    "\n",
    "    # mapstat\n",
    "    mapstat = {}\n",
    "    mapstat['map'] = match_info['map']\n",
    "    left = contentCol.find('div', attrs={'class': 'team-left'})\n",
    "    right = contentCol.find('div', attrs={'class': 'team-right'})\n",
    "    mapstat['total_rounds'] = int(left.div.text) + int(right.div.text)\n",
    "    if (int(left.div.text) >= int(right.div.text)):\n",
    "        winner = left\n",
    "    else:\n",
    "        winner = right\n",
    "    mapstat['winner'] = get_team_name(winner.a)\n",
    "    mapstat['winner_id'] = get_team_id(winner.a)\n",
    "    mapstat['win_rounds'] = winner.div.text\n",
    "\n",
    "    # mapstatid\n",
    "    mapstat['mapstatid'] = re.findall(\"/mapstatsid/(\\d+)/\", match_info['href'])[0]\n",
    "    # scrape all others\n",
    "    mapstat.update(map_match_scrape(contentCol))\n",
    "\n",
    "    match_result['mapstat'] = mapstat\n",
    "    total = time.time() - start_time\n",
    "    # print(f'wait={wait_time}, total={total}')\n",
    "    return match_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert scrape result to pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_padding(df, column_name, pad):\n",
    "    df[column_name] = [pad] * df.shape[0]\n",
    "    return df\n",
    "\n",
    "def match_stat_df_get(match_res):\n",
    "    # define the order.\n",
    "    column_names = ['player_id','player','match_id','date',\n",
    "                'kills', 'hs', 'assists', 'flash_assists', 'kdratio', 'deaths', 'kddiff', 'adr', 'fkdiff']\n",
    "    player_match_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "#    for mapstat in match_res['mapstat']:\n",
    "    mapstat = match_res['mapstat']\n",
    "    if mapstat:\n",
    "        player_map_df = pd.read_json(json.dumps(mapstat['stat']), orient='index')\n",
    "        # padding\n",
    "        player_map_df = df_padding(player_map_df, 'match_id', match_res['match_id'])\n",
    "        player_map_df = df_padding(player_map_df, 'winner', mapstat['winner'])\n",
    "        player_map_df = df_padding(player_map_df, 'winner_id', mapstat['winner_id'])\n",
    "        \n",
    "        player_map_df = df_padding(player_map_df, 'total_rounds', mapstat['total_rounds'])\n",
    "        player_map_df = df_padding(player_map_df, 'mapstatid', mapstat['mapstatid'])\n",
    "        \n",
    "        player_map_df = df_padding(player_map_df, 'win_rounds', mapstat['win_rounds'])\n",
    "        player_map_df = df_padding(player_map_df, 'map', mapstat['map'])\n",
    "        \n",
    "        t_str = datetime.utcfromtimestamp(match_res[key_timestamp]).strftime('%Y-%m-%d')\n",
    "        player_map_df = df_padding(player_map_df, 'date', t_str)\n",
    "\n",
    "        player_match_df = pd.concat([player_match_df, player_map_df])\n",
    "    return player_match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_column_names = [\n",
    "     'player_id',\n",
    "     'player',\n",
    "     'match_id',\n",
    "     'date',\n",
    "     'kills',\n",
    "     'hs',\n",
    "     'assists',\n",
    "     'flash_assists',\n",
    "     'kdratio',\n",
    "     'deaths',\n",
    "     'kddiff',\n",
    "     'adr',\n",
    "     'fkdiff',\n",
    "     'rating',\n",
    "     'teamname',\n",
    "     'winner',\n",
    "     'winner_id',\n",
    "     'total_rounds',\n",
    "     'mapstatid',\n",
    "     'win_rounds',\n",
    "     'map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_scrape(browser, match_urls):\n",
    "    if len(match_urls) == 0:\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 1st url's time pending to the output filename\n",
    "    ts = match_urls[0]['ts']\n",
    "    t1 = datetime.utcfromtimestamp(ts).strftime('%Y%m%d')\n",
    "    # last url's time pending to the output filename\n",
    "    ts = match_urls[-1]['ts']\n",
    "    t2 = datetime.utcfromtimestamp(ts).strftime('%Y%m%d')\n",
    "    output_filename = 'player_mapstat_' + t1 + '_' + t2 + '.csv'\n",
    "\n",
    "    # create an empty file\n",
    "    df = pd.DataFrame(columns=output_df_column_names)\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    # append the file.\n",
    "    count = 0\n",
    "    for match_result in match_urls:\n",
    "        res = match_scrape(browser, match_result)\n",
    "        if len(res) == 0:\n",
    "            continue\n",
    "        df = match_stat_df_get(res)\n",
    "        df[output_df_column_names].to_csv(output_filename, mode='a', header=False)\n",
    "        count += 1\n",
    "        if ((count % 20) == 0):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'scraped {count} matches in {elapsed_time} seconds.')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Done. scraped {count} matches in {elapsed_time} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config and run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for match_result in match_urls:\n",
    "    mapstatid = mapstatsid_get(match_result['href'])\n",
    "    mapstatsid_pend(mapstatid)\n",
    "len(scrapped_mapstatsid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "match_result = match_urls[0]\n",
    "mapstatid = mapstatsid_get(match_result['href'])\n",
    "is_mapstatsid_scraped(mapstatid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\n",
    "#    '2017/1/1',\n",
    "#    '2016/12/1',\n",
    "#    '2016/11/1',\n",
    "#    '2016/10/1',\n",
    "    '2016/9/15',\n",
    "    '2016/9/1',\n",
    "    '2016/8/1',\n",
    "    '2016/7/1',\n",
    "    '2016/6/1',\n",
    "    '2016/5/1',\n",
    "    '2016/4/1',\n",
    "    '2016/3/1',\n",
    "    '2016/2/1',\n",
    "    '2016/1/1',\n",
    "]\n",
    "#next_url = '/stats/matches'\n",
    "next_url = '/stats/matches?offset=60500'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dates = [\n",
    "    '2019/12/5',\n",
    "    '2019/12/1',\n",
    "]\n",
    "#next_url = '/stats/matches'\n",
    "#next_url = '/stats/matches?offset=21400'\n",
    "next_url = '/stats/matches?offset=7000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now 06/22/2020, 09:46:52\n",
      "Included 1st match timestamp: 2016/09/13 02:30:00\n",
      "Excluded 1st match timestamp: 2016/09/01 03:15:00\n",
      "total 481, next_url=/stats/matches?offset=60950\n",
      "\n",
      "\n",
      "scraped 20 matches in 20.356404304504395 seconds.\n",
      "exception: href=/stats/matches/mapstatsid/35104/selfless-vs-prospects\n",
      "   match_page_link=<a class=\"match-page-link button\">More info on match page</a>\n",
      "exception: href=/stats/matches/mapstatsid/35103/prospects-vs-selfless\n",
      "   match_page_link=<a class=\"match-page-link button\">More info on match page</a>\n",
      "scraped 40 matches in 45.65940546989441 seconds.\n",
      "scraped 60 matches in 68.93723464012146 seconds.\n",
      "scraped 80 matches in 93.44985866546631 seconds.\n",
      "scraped 100 matches in 116.42384004592896 seconds.\n",
      "scraped 120 matches in 139.12050032615662 seconds.\n",
      "scraped 140 matches in 163.63787150382996 seconds.\n",
      "scraped 160 matches in 188.99546122550964 seconds.\n",
      "scraped 180 matches in 214.67884016036987 seconds.\n",
      "scraped 200 matches in 239.27339100837708 seconds.\n",
      "scraped 220 matches in 263.5007245540619 seconds.\n",
      "scraped 240 matches in 290.65565490722656 seconds.\n",
      "scraped 260 matches in 320.8656265735626 seconds.\n",
      "scraped 280 matches in 347.06745052337646 seconds.\n",
      "scraped 300 matches in 371.95302534103394 seconds.\n",
      "scraped 320 matches in 396.53334498405457 seconds.\n",
      "scraped 340 matches in 419.66700196266174 seconds.\n",
      "scraped 360 matches in 444.65047311782837 seconds.\n",
      "scraped 380 matches in 470.7022340297699 seconds.\n",
      "scraped 400 matches in 496.008994102478 seconds.\n",
      "scraped 420 matches in 522.0384764671326 seconds.\n",
      "scraped 440 matches in 549.0506534576416 seconds.\n",
      "scraped 460 matches in 575.7380256652832 seconds.\n",
      "Done. scraped 479 matches in 601.3914680480957 seconds.\n",
      "now 06/22/2020, 09:57:06\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/09/01 03:15:00\n",
      "Excluded 1st match timestamp: 2016/07/31 22:45:00\n",
      "total 1074, next_url=/stats/matches?offset=62050\n",
      "\n",
      "\n",
      "scraped 20 matches in 28.884961366653442 seconds.\n",
      "scraped 40 matches in 54.53317832946777 seconds.\n",
      "scraped 60 matches in 78.9726402759552 seconds.\n",
      "scraped 80 matches in 104.98549389839172 seconds.\n",
      "scraped 100 matches in 130.43846035003662 seconds.\n",
      "scraped 120 matches in 156.02145075798035 seconds.\n",
      "scraped 140 matches in 182.59399223327637 seconds.\n",
      "scraped 160 matches in 210.34510684013367 seconds.\n",
      "scraped 180 matches in 239.04704427719116 seconds.\n",
      "scraped 200 matches in 266.89796352386475 seconds.\n",
      "scraped 220 matches in 293.201491355896 seconds.\n",
      "scraped 240 matches in 320.21059560775757 seconds.\n",
      "scraped 260 matches in 345.9393718242645 seconds.\n",
      "scraped 280 matches in 373.51144456863403 seconds.\n",
      "scraped 300 matches in 399.7870831489563 seconds.\n",
      "scraped 320 matches in 426.3229908943176 seconds.\n",
      "scraped 340 matches in 451.86185789108276 seconds.\n",
      "scraped 360 matches in 479.4384083747864 seconds.\n",
      "scraped 380 matches in 508.09062147140503 seconds.\n",
      "scraped 400 matches in 536.5687205791473 seconds.\n",
      "scraped 420 matches in 566.8086378574371 seconds.\n",
      "scraped 440 matches in 594.7047538757324 seconds.\n",
      "scraped 460 matches in 622.1387002468109 seconds.\n",
      "scraped 480 matches in 649.8341958522797 seconds.\n",
      "scraped 500 matches in 678.781977891922 seconds.\n",
      "scraped 520 matches in 705.6163430213928 seconds.\n",
      "scraped 540 matches in 732.3309965133667 seconds.\n",
      "scraped 560 matches in 759.8964974880219 seconds.\n",
      "scraped 580 matches in 787.9737989902496 seconds.\n",
      "scraped 600 matches in 816.6647572517395 seconds.\n",
      "scraped 620 matches in 844.4701383113861 seconds.\n",
      "scraped 640 matches in 873.8627924919128 seconds.\n",
      "scraped 660 matches in 901.8860764503479 seconds.\n",
      "scraped 680 matches in 930.777884721756 seconds.\n",
      "scraped 700 matches in 962.4363765716553 seconds.\n",
      "scraped 720 matches in 989.5484888553619 seconds.\n",
      "scraped 740 matches in 1017.8789296150208 seconds.\n",
      "scraped 760 matches in 1049.8979420661926 seconds.\n",
      "scraped 780 matches in 1081.2736694812775 seconds.\n",
      "scraped 800 matches in 1119.9552063941956 seconds.\n",
      "scraped 820 matches in 1156.9132800102234 seconds.\n",
      "scraped 840 matches in 1187.2139070034027 seconds.\n",
      "scraped 860 matches in 1222.0144057273865 seconds.\n",
      "scraped 880 matches in 1252.1495554447174 seconds.\n",
      "scraped 900 matches in 1282.3111653327942 seconds.\n",
      "scraped 920 matches in 1311.8926663398743 seconds.\n",
      "scraped 940 matches in 1342.6150863170624 seconds.\n",
      "scraped 960 matches in 1373.3444952964783 seconds.\n",
      "scraped 980 matches in 1402.281477689743 seconds.\n",
      "scraped 1000 matches in 1433.007784128189 seconds.\n",
      "scraped 1020 matches in 1464.7212948799133 seconds.\n",
      "scraped 1040 matches in 1498.6705968379974 seconds.\n",
      "scraped 1060 matches in 1529.7294247150421 seconds.\n",
      "Done. scraped 1074 matches in 1552.5698807239532 seconds.\n",
      "now 06/22/2020, 10:23:36\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/07/31 22:45:00\n",
      "Excluded 1st match timestamp: 2016/07/01 03:00:00\n",
      "total 808, next_url=/stats/matches?offset=62850\n",
      "\n",
      "\n",
      "scraped 20 matches in 30.286041259765625 seconds.\n",
      "scraped 40 matches in 62.05273127555847 seconds.\n",
      "scraped 60 matches in 90.9750428199768 seconds.\n",
      "scraped 80 matches in 121.57714104652405 seconds.\n",
      "scraped 100 matches in 151.81015634536743 seconds.\n",
      "scraped 120 matches in 181.56141424179077 seconds.\n",
      "scraped 140 matches in 210.38213539123535 seconds.\n",
      "scraped 160 matches in 241.22353219985962 seconds.\n",
      "scraped 180 matches in 270.93206238746643 seconds.\n",
      "scraped 200 matches in 303.04788517951965 seconds.\n",
      "scraped 220 matches in 335.7853603363037 seconds.\n",
      "scraped 240 matches in 369.1139245033264 seconds.\n",
      "scraped 260 matches in 405.01389718055725 seconds.\n",
      "scraped 280 matches in 444.95923113822937 seconds.\n",
      "scraped 300 matches in 478.8579857349396 seconds.\n",
      "scraped 320 matches in 510.18033242225647 seconds.\n",
      "scraped 340 matches in 541.0714769363403 seconds.\n",
      "scraped 360 matches in 572.3413870334625 seconds.\n",
      "scraped 380 matches in 606.6702547073364 seconds.\n",
      "scraped 400 matches in 643.2835636138916 seconds.\n",
      "scraped 420 matches in 685.4719762802124 seconds.\n",
      "scraped 440 matches in 727.2544555664062 seconds.\n",
      "scraped 460 matches in 767.840322971344 seconds.\n",
      "scraped 480 matches in 801.6194384098053 seconds.\n",
      "scraped 500 matches in 838.7079553604126 seconds.\n",
      "scraped 520 matches in 878.9859495162964 seconds.\n",
      "scraped 540 matches in 917.2271795272827 seconds.\n",
      "scraped 560 matches in 951.7815780639648 seconds.\n",
      "scraped 580 matches in 986.0577077865601 seconds.\n",
      "scraped 600 matches in 1017.4093511104584 seconds.\n",
      "scraped 620 matches in 1053.1196722984314 seconds.\n",
      "scraped 640 matches in 1086.7964522838593 seconds.\n",
      "scraped 660 matches in 1123.6359095573425 seconds.\n",
      "scraped 680 matches in 1154.1910030841827 seconds.\n",
      "scraped 700 matches in 1183.567569732666 seconds.\n",
      "scraped 720 matches in 1215.9650967121124 seconds.\n",
      "scraped 740 matches in 1248.2094049453735 seconds.\n",
      "scraped 760 matches in 1281.3967382907867 seconds.\n",
      "scraped 780 matches in 1313.178909778595 seconds.\n",
      "scraped 800 matches in 1348.2071578502655 seconds.\n",
      "Done. scraped 808 matches in 1361.622316122055 seconds.\n",
      "now 06/22/2020, 10:46:53\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/07/01 03:00:00\n",
      "Excluded 1st match timestamp: 2016/06/01 00:00:00\n",
      "total 702, next_url=/stats/matches?offset=63550\n",
      "\n",
      "\n",
      "scraped 20 matches in 29.156400680541992 seconds.\n",
      "scraped 40 matches in 58.83907914161682 seconds.\n",
      "scraped 60 matches in 91.64060521125793 seconds.\n",
      "scraped 80 matches in 122.37399435043335 seconds.\n",
      "scraped 100 matches in 151.85721015930176 seconds.\n",
      "scraped 120 matches in 183.88037014007568 seconds.\n",
      "scraped 140 matches in 213.9954080581665 seconds.\n",
      "scraped 160 matches in 245.87054228782654 seconds.\n",
      "scraped 180 matches in 280.8238182067871 seconds.\n",
      "scraped 200 matches in 316.79201078414917 seconds.\n",
      "scraped 220 matches in 355.45235228538513 seconds.\n",
      "scraped 240 matches in 395.6958968639374 seconds.\n",
      "scraped 260 matches in 427.9349956512451 seconds.\n",
      "scraped 280 matches in 458.96699023246765 seconds.\n",
      "scraped 300 matches in 490.42532896995544 seconds.\n",
      "scraped 320 matches in 520.1617183685303 seconds.\n",
      "scraped 340 matches in 553.0213413238525 seconds.\n",
      "scraped 360 matches in 583.8215095996857 seconds.\n",
      "scraped 380 matches in 616.039998292923 seconds.\n",
      "scraped 400 matches in 647.6869659423828 seconds.\n",
      "scraped 420 matches in 676.664208650589 seconds.\n",
      "scraped 440 matches in 706.6535563468933 seconds.\n",
      "scraped 460 matches in 735.9913830757141 seconds.\n",
      "scraped 480 matches in 766.4287581443787 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 500 matches in 797.0614252090454 seconds.\n",
      "scraped 520 matches in 824.446540594101 seconds.\n",
      "scraped 540 matches in 854.448207616806 seconds.\n",
      "scraped 560 matches in 885.7122383117676 seconds.\n",
      "scraped 580 matches in 916.3217477798462 seconds.\n",
      "scraped 600 matches in 947.6625578403473 seconds.\n",
      "scraped 620 matches in 978.1076319217682 seconds.\n",
      "scraped 640 matches in 1008.324684381485 seconds.\n",
      "scraped 660 matches in 1037.7654695510864 seconds.\n",
      "scraped 680 matches in 1068.0549824237823 seconds.\n",
      "scraped 700 matches in 1097.5023412704468 seconds.\n",
      "Done. scraped 702 matches in 1099.9854834079742 seconds.\n",
      "now 06/22/2020, 11:05:41\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/06/01 00:00:00\n",
      "Excluded 1st match timestamp: 2016/05/01 02:00:00\n",
      "total 862, next_url=/stats/matches?offset=64400\n",
      "\n",
      "\n",
      "scraped 20 matches in 31.6463725566864 seconds.\n",
      "scraped 40 matches in 69.34147477149963 seconds.\n",
      "scraped 60 matches in 105.19058585166931 seconds.\n",
      "scraped 80 matches in 144.99957966804504 seconds.\n",
      "scraped 100 matches in 194.27980732917786 seconds.\n",
      "scraped 120 matches in 235.17491030693054 seconds.\n",
      "scraped 140 matches in 267.13261127471924 seconds.\n",
      "scraped 160 matches in 299.29918909072876 seconds.\n",
      "scraped 180 matches in 332.5617723464966 seconds.\n",
      "scraped 200 matches in 369.5671510696411 seconds.\n",
      "scraped 220 matches in 401.9721910953522 seconds.\n",
      "scraped 240 matches in 434.9651699066162 seconds.\n",
      "scraped 260 matches in 467.72549510002136 seconds.\n",
      "scraped 280 matches in 503.8856499195099 seconds.\n",
      "scraped 300 matches in 538.0371444225311 seconds.\n",
      "scraped 320 matches in 571.6741218566895 seconds.\n",
      "scraped 340 matches in 606.9413886070251 seconds.\n",
      "scraped 360 matches in 640.6400525569916 seconds.\n",
      "scraped 380 matches in 673.1920335292816 seconds.\n",
      "scraped 400 matches in 707.1234159469604 seconds.\n",
      "scraped 420 matches in 742.6374957561493 seconds.\n",
      "scraped 440 matches in 774.5844902992249 seconds.\n",
      "scraped 460 matches in 808.1992926597595 seconds.\n",
      "scraped 480 matches in 842.6703052520752 seconds.\n",
      "scraped 500 matches in 877.17054438591 seconds.\n",
      "scraped 520 matches in 908.3859713077545 seconds.\n",
      "scraped 540 matches in 940.4797751903534 seconds.\n",
      "scraped 560 matches in 973.6663179397583 seconds.\n",
      "scraped 580 matches in 1007.26420545578 seconds.\n",
      "scraped 600 matches in 1045.0535471439362 seconds.\n",
      "scraped 620 matches in 1077.4448277950287 seconds.\n",
      "scraped 640 matches in 1110.9063789844513 seconds.\n",
      "scraped 660 matches in 1147.104245185852 seconds.\n",
      "scraped 680 matches in 1182.0878646373749 seconds.\n",
      "scraped 700 matches in 1215.2035465240479 seconds.\n",
      "scraped 720 matches in 1251.745994567871 seconds.\n",
      "scraped 740 matches in 1286.1352059841156 seconds.\n",
      "scraped 760 matches in 1320.940054178238 seconds.\n",
      "scraped 780 matches in 1353.8007140159607 seconds.\n",
      "scraped 800 matches in 1387.4267482757568 seconds.\n",
      "scraped 820 matches in 1420.6211960315704 seconds.\n",
      "scraped 840 matches in 1449.961814880371 seconds.\n",
      "scraped 860 matches in 1484.2803778648376 seconds.\n",
      "Done. scraped 862 matches in 1488.287582397461 seconds.\n",
      "now 06/22/2020, 11:31:03\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/05/01 02:00:00\n",
      "Excluded 1st match timestamp: 2016/04/01 00:00:00\n",
      "total 880, next_url=/stats/matches?offset=65300\n",
      "\n",
      "\n",
      "scraped 20 matches in 31.334198236465454 seconds.\n",
      "scraped 40 matches in 66.01279377937317 seconds.\n",
      "scraped 60 matches in 103.4590814113617 seconds.\n",
      "scraped 80 matches in 139.01787996292114 seconds.\n",
      "scraped 100 matches in 180.77258920669556 seconds.\n",
      "scraped 120 matches in 215.12733507156372 seconds.\n",
      "scraped 140 matches in 248.59921860694885 seconds.\n",
      "scraped 160 matches in 281.30608677864075 seconds.\n",
      "scraped 180 matches in 313.0391836166382 seconds.\n",
      "scraped 200 matches in 345.66768860816956 seconds.\n",
      "scraped 220 matches in 377.6157352924347 seconds.\n",
      "scraped 240 matches in 412.2902717590332 seconds.\n",
      "scraped 260 matches in 443.6102557182312 seconds.\n",
      "scraped 280 matches in 477.43912959098816 seconds.\n",
      "scraped 300 matches in 508.1928050518036 seconds.\n",
      "scraped 320 matches in 541.5314302444458 seconds.\n",
      "scraped 340 matches in 573.4053854942322 seconds.\n",
      "scraped 360 matches in 608.3825829029083 seconds.\n",
      "scraped 380 matches in 647.5284199714661 seconds.\n",
      "scraped 400 matches in 683.5475780963898 seconds.\n",
      "scraped 420 matches in 721.4026930332184 seconds.\n",
      "scraped 440 matches in 755.4439959526062 seconds.\n",
      "scraped 460 matches in 788.5307462215424 seconds.\n",
      "scraped 480 matches in 821.8462648391724 seconds.\n",
      "scraped 500 matches in 853.71621966362 seconds.\n",
      "scraped 520 matches in 886.0307247638702 seconds.\n",
      "scraped 540 matches in 918.4988875389099 seconds.\n",
      "scraped 560 matches in 953.1158895492554 seconds.\n",
      "scraped 580 matches in 988.7075870037079 seconds.\n",
      "scraped 600 matches in 1023.5865352153778 seconds.\n",
      "scraped 620 matches in 1057.2016379833221 seconds.\n",
      "scraped 640 matches in 1091.5934629440308 seconds.\n",
      "scraped 660 matches in 1126.9341359138489 seconds.\n",
      "scraped 680 matches in 1163.0009877681732 seconds.\n",
      "scraped 700 matches in 1199.50168466568 seconds.\n",
      "scraped 720 matches in 1234.2890763282776 seconds.\n",
      "scraped 740 matches in 1270.1288104057312 seconds.\n",
      "scraped 760 matches in 1305.3185768127441 seconds.\n",
      "scraped 780 matches in 1342.084751367569 seconds.\n",
      "scraped 800 matches in 1375.6708433628082 seconds.\n",
      "scraped 820 matches in 1410.2841432094574 seconds.\n",
      "scraped 840 matches in 1444.1748101711273 seconds.\n",
      "scraped 860 matches in 1480.833396434784 seconds.\n",
      "scraped 880 matches in 1516.0499913692474 seconds.\n",
      "Done. scraped 880 matches in 1516.0509886741638 seconds.\n",
      "now 06/22/2020, 11:56:56\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/04/01 00:00:00\n",
      "Excluded 1st match timestamp: 2016/03/01 03:45:00\n",
      "total 1050, next_url=/stats/matches?offset=66350\n",
      "\n",
      "\n",
      "scraped 20 matches in 31.653615474700928 seconds.\n",
      "scraped 40 matches in 66.74115180969238 seconds.\n",
      "scraped 60 matches in 105.9486448764801 seconds.\n",
      "scraped 80 matches in 148.7943263053894 seconds.\n",
      "scraped 100 matches in 194.02492833137512 seconds.\n",
      "scraped 120 matches in 227.38348484039307 seconds.\n",
      "scraped 140 matches in 262.8621428012848 seconds.\n",
      "scraped 160 matches in 296.0364134311676 seconds.\n",
      "scraped 180 matches in 331.83928179740906 seconds.\n",
      "scraped 200 matches in 366.25861287117004 seconds.\n",
      "scraped 220 matches in 400.5169680118561 seconds.\n",
      "scraped 240 matches in 434.4813838005066 seconds.\n",
      "scraped 260 matches in 469.5836865901947 seconds.\n",
      "scraped 280 matches in 502.7075283527374 seconds.\n",
      "scraped 300 matches in 538.0262446403503 seconds.\n",
      "scraped 320 matches in 574.5071244239807 seconds.\n",
      "scraped 340 matches in 609.85613322258 seconds.\n",
      "scraped 360 matches in 643.6525750160217 seconds.\n",
      "scraped 380 matches in 678.209879398346 seconds.\n",
      "scraped 400 matches in 714.0397245883942 seconds.\n",
      "scraped 420 matches in 748.8165299892426 seconds.\n",
      "scraped 440 matches in 782.4670231342316 seconds.\n",
      "scraped 460 matches in 817.6800663471222 seconds.\n",
      "scraped 480 matches in 853.1117055416107 seconds.\n",
      "scraped 500 matches in 886.7296800613403 seconds.\n",
      "scraped 520 matches in 921.6617891788483 seconds.\n",
      "scraped 540 matches in 956.0024125576019 seconds.\n",
      "scraped 560 matches in 990.875659942627 seconds.\n",
      "scraped 580 matches in 1027.66526389122 seconds.\n",
      "scraped 600 matches in 1060.0379195213318 seconds.\n",
      "scraped 620 matches in 1093.7988941669464 seconds.\n",
      "scraped 640 matches in 1131.390478849411 seconds.\n",
      "scraped 660 matches in 1165.6243834495544 seconds.\n",
      "scraped 680 matches in 1201.4690775871277 seconds.\n",
      "scraped 700 matches in 1239.4157557487488 seconds.\n",
      "scraped 720 matches in 1276.9838564395905 seconds.\n",
      "scraped 740 matches in 1312.169436454773 seconds.\n",
      "scraped 760 matches in 1346.6663191318512 seconds.\n",
      "scraped 780 matches in 1382.8896696567535 seconds.\n",
      "scraped 800 matches in 1419.6603412628174 seconds.\n",
      "scraped 820 matches in 1455.3709061145782 seconds.\n",
      "scraped 840 matches in 1492.3126389980316 seconds.\n",
      "scraped 860 matches in 1527.9262087345123 seconds.\n",
      "scraped 880 matches in 1562.930751800537 seconds.\n",
      "scraped 900 matches in 1598.9709627628326 seconds.\n",
      "scraped 920 matches in 1635.058725118637 seconds.\n",
      "scraped 940 matches in 1669.4533638954163 seconds.\n",
      "scraped 960 matches in 1706.4605391025543 seconds.\n",
      "scraped 980 matches in 1741.8230686187744 seconds.\n",
      "scraped 1000 matches in 1777.785415649414 seconds.\n",
      "scraped 1020 matches in 1812.9510951042175 seconds.\n",
      "scraped 1040 matches in 1849.096314907074 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. scraped 1050 matches in 1868.2878139019012 seconds.\n",
      "now 06/22/2020, 12:28:47\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/03/01 03:45:00\n",
      "Excluded 1st match timestamp: 2016/02/01 05:30:00\n",
      "total 891, next_url=/stats/matches?offset=67250\n",
      "\n",
      "\n",
      "scraped 20 matches in 34.23291897773743 seconds.\n",
      "scraped 40 matches in 73.62267112731934 seconds.\n",
      "scraped 60 matches in 114.1174647808075 seconds.\n",
      "scraped 80 matches in 158.40781593322754 seconds.\n",
      "scraped 100 matches in 196.05120944976807 seconds.\n",
      "scraped 120 matches in 228.43134927749634 seconds.\n",
      "scraped 140 matches in 262.9115786552429 seconds.\n",
      "scraped 160 matches in 295.6639199256897 seconds.\n",
      "scraped 180 matches in 330.1697075366974 seconds.\n",
      "scraped 200 matches in 366.244211435318 seconds.\n",
      "scraped 220 matches in 400.0146098136902 seconds.\n",
      "scraped 240 matches in 433.5201983451843 seconds.\n",
      "scraped 260 matches in 468.64870595932007 seconds.\n",
      "scraped 280 matches in 503.75652551651 seconds.\n",
      "scraped 300 matches in 538.5182464122772 seconds.\n",
      "scraped 320 matches in 575.5169107913971 seconds.\n",
      "scraped 340 matches in 613.1014428138733 seconds.\n",
      "scraped 360 matches in 654.78906416893 seconds.\n",
      "scraped 380 matches in 695.6278789043427 seconds.\n",
      "scraped 400 matches in 731.1296210289001 seconds.\n",
      "scraped 420 matches in 767.1832046508789 seconds.\n",
      "scraped 440 matches in 802.8543584346771 seconds.\n",
      "scraped 460 matches in 837.5217208862305 seconds.\n",
      "scraped 480 matches in 873.350781917572 seconds.\n",
      "scraped 500 matches in 908.2352848052979 seconds.\n",
      "scraped 520 matches in 942.0274548530579 seconds.\n",
      "scraped 540 matches in 976.9184923171997 seconds.\n",
      "scraped 560 matches in 1011.3423955440521 seconds.\n",
      "scraped 580 matches in 1045.7939026355743 seconds.\n",
      "scraped 600 matches in 1081.8327231407166 seconds.\n",
      "scraped 620 matches in 1118.7263808250427 seconds.\n",
      "scraped 640 matches in 1156.3679947853088 seconds.\n",
      "scraped 660 matches in 1192.4949312210083 seconds.\n",
      "scraped 680 matches in 1229.9879314899445 seconds.\n",
      "scraped 700 matches in 1272.2117838859558 seconds.\n",
      "scraped 720 matches in 1309.836651802063 seconds.\n",
      "scraped 740 matches in 1347.5931360721588 seconds.\n",
      "scraped 760 matches in 1382.9727792739868 seconds.\n",
      "scraped 780 matches in 1421.5213429927826 seconds.\n",
      "scraped 800 matches in 1465.6968131065369 seconds.\n",
      "scraped 820 matches in 1503.1876757144928 seconds.\n",
      "scraped 840 matches in 1539.090300321579 seconds.\n",
      "scraped 860 matches in 1575.0808308124542 seconds.\n",
      "scraped 880 matches in 1611.1270077228546 seconds.\n",
      "Done. scraped 891 matches in 1632.8435635566711 seconds.\n",
      "now 06/22/2020, 12:56:39\n",
      "\n",
      "\n",
      "Included 1st match timestamp: 2016/02/01 05:30:00\n",
      "Excluded 1st match timestamp: 2015/12/22 21:15:00\n",
      "total 515, next_url=/stats/matches?offset=67750\n",
      "\n",
      "\n",
      "scraped 20 matches in 36.00498151779175 seconds.\n",
      "scraped 40 matches in 75.6792984008789 seconds.\n",
      "scraped 60 matches in 111.90320086479187 seconds.\n",
      "scraped 80 matches in 152.00536227226257 seconds.\n",
      "scraped 100 matches in 193.19716358184814 seconds.\n",
      "scraped 120 matches in 243.89194583892822 seconds.\n",
      "scraped 140 matches in 278.4190514087677 seconds.\n",
      "scraped 160 matches in 316.48713850975037 seconds.\n",
      "scraped 180 matches in 355.74831199645996 seconds.\n",
      "scraped 200 matches in 396.13563990592957 seconds.\n",
      "scraped 220 matches in 432.07511258125305 seconds.\n",
      "scraped 240 matches in 470.01857113838196 seconds.\n",
      "scraped 260 matches in 509.7678701877594 seconds.\n",
      "scraped 280 matches in 551.2915861606598 seconds.\n",
      "scraped 300 matches in 588.4151265621185 seconds.\n",
      "scraped 320 matches in 628.3804218769073 seconds.\n",
      "scraped 340 matches in 668.730327129364 seconds.\n",
      "scraped 360 matches in 708.1621744632721 seconds.\n",
      "scraped 380 matches in 752.4603636264801 seconds.\n",
      "scraped 400 matches in 797.589371919632 seconds.\n",
      "scraped 420 matches in 832.3118131160736 seconds.\n",
      "scraped 440 matches in 874.509149312973 seconds.\n",
      "scraped 460 matches in 920.1251330375671 seconds.\n",
      "scraped 480 matches in 953.3195097446442 seconds.\n",
      "scraped 500 matches in 993.5405719280243 seconds.\n",
      "Done. scraped 515 matches in 1022.1654722690582 seconds.\n",
      "now 06/22/2020, 13:14:06\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current date and time\n",
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print(f\"now {now_str}\")\n",
    "\n",
    "start_date = None\n",
    "for stop_date in dates:\n",
    "    if not start_date:\n",
    "        start_date = stop_date\n",
    "        continue\n",
    "    start_ts = datetime.timestamp(datetime.strptime(start_date, \"%Y/%m/%d\"))\n",
    "    stop_ts = datetime.timestamp(datetime.strptime(stop_date, \"%Y/%m/%d\"))\n",
    "\n",
    "    match_urls, next_url = match_list_get(browser, start_ts, stop_ts, next_url)\n",
    "    print(f'total {len(match_urls)}, next_url={next_url}\\n\\n')\n",
    "\n",
    "\n",
    "    match_list_scrape(browser, match_urls)\n",
    "\n",
    "#    print(f'{start_date}, {stop_date}')\n",
    "    start_date = stop_date\n",
    "\n",
    "    # current date and time\n",
    "    now = datetime.now()\n",
    "    now_str = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    print(f\"now {now_str}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
